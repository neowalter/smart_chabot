{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co83heYDRgTC"
      },
      "source": [
        "# Enviroment setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFA7Tj4iRX1c",
        "outputId": "8e01ebb9-35ca-4d5c-9a43-1c151abcba53"
      },
      "source": [
        "!nvidia-smi\n",
        "# make sure having the GPU running"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar 19 02:18:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcV8P_23R-QC",
        "outputId": "38cde894-c069-440d-b70d-f1de18c1b478"
      },
      "source": [
        "%%time\n",
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt \n",
        "# PyTorch etc"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 18.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.3MB 58.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 62.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 63.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 61.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 51.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 63.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 62.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 55.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 63.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[?25h  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: omegaconf 2.0.6 has requirement PyYAML>=5.1.*, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.12.10 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210317 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NoNMkauSiwk"
      },
      "source": [
        "# Quick view the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uKTIRS5aSTrY",
        "outputId": "a7ed5ddf-f5a1-4bc8-c682-af73c10cb982"
      },
      "source": [
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "Interactive.main(\n",
        "\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:21:36 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "02:21:36 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:50<00:00, 22.1MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:22:46 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
            "02:22:46 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "02:22:46 | Using CUDA\n",
            "02:22:46 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "02:22:46 | num words = 54944\n",
            "02:22:46 | TransformerGenerator: full interactive mode on.\n",
            "02:22:47 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "02:22:58 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:22:58 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:22:59 | Opt:\n",
            "02:22:59 |     activation: gelu\n",
            "02:22:59 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:22:59 |     adam_eps: 1e-06\n",
            "02:22:59 |     add_p1_after_newln: False\n",
            "02:22:59 |     aggregate_micro: False\n",
            "02:22:59 |     allow_missing_init_opts: False\n",
            "02:22:59 |     attention_dropout: 0.0\n",
            "02:22:59 |     batch_length_range: 5\n",
            "02:22:59 |     batch_sort_cache_type: pop\n",
            "02:22:59 |     batch_sort_field: text\n",
            "02:22:59 |     batchsize: 48\n",
            "02:22:59 |     beam_block_full_context: False\n",
            "02:22:59 |     beam_block_list_filename: None\n",
            "02:22:59 |     beam_block_ngram: 3\n",
            "02:22:59 |     beam_context_block_ngram: 3\n",
            "02:22:59 |     beam_delay: 30\n",
            "02:22:59 |     beam_length_penalty: 0.65\n",
            "02:22:59 |     beam_min_length: 10\n",
            "02:22:59 |     beam_min_n_best: 3\n",
            "02:22:59 |     beam_size: 8\n",
            "02:22:59 |     betas: '[0.9, 0.98]'\n",
            "02:22:59 |     bpe_add_prefix_space: None\n",
            "02:22:59 |     bpe_debug: False\n",
            "02:22:59 |     bpe_dropout: None\n",
            "02:22:59 |     bpe_merge: None\n",
            "02:22:59 |     bpe_vocab: None\n",
            "02:22:59 |     compute_tokenized_bleu: False\n",
            "02:22:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:22:59 |     datatype: train:stream\n",
            "02:22:59 |     delimiter: '\\n'\n",
            "02:22:59 |     dict_build_first: True\n",
            "02:22:59 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:22:59 |     dict_endtoken: __end__\n",
            "02:22:59 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "02:22:59 |     dict_include_test: False\n",
            "02:22:59 |     dict_include_valid: False\n",
            "02:22:59 |     dict_initpath: None\n",
            "02:22:59 |     dict_language: english\n",
            "02:22:59 |     dict_loaded: True\n",
            "02:22:59 |     dict_lower: True\n",
            "02:22:59 |     dict_max_ngram_size: -1\n",
            "02:22:59 |     dict_maxexs: -1\n",
            "02:22:59 |     dict_maxtokens: -1\n",
            "02:22:59 |     dict_minfreq: 0\n",
            "02:22:59 |     dict_nulltoken: __null__\n",
            "02:22:59 |     dict_starttoken: __start__\n",
            "02:22:59 |     dict_textfields: text,labels\n",
            "02:22:59 |     dict_tokenizer: bpe\n",
            "02:22:59 |     dict_unktoken: __unk__\n",
            "02:22:59 |     display_add_fields: \n",
            "02:22:59 |     display_examples: False\n",
            "02:22:59 |     display_prettify: False\n",
            "02:22:59 |     distributed_world_size: 64\n",
            "02:22:59 |     download_path: None\n",
            "02:22:59 |     dropout: 0.1\n",
            "02:22:59 |     dynamic_batching: None\n",
            "02:22:59 |     embedding_projection: random\n",
            "02:22:59 |     embedding_size: 512\n",
            "02:22:59 |     embedding_type: random\n",
            "02:22:59 |     embeddings_scale: True\n",
            "02:22:59 |     eval_batchsize: None\n",
            "02:22:59 |     evaltask: None\n",
            "02:22:59 |     ffn_size: 2048\n",
            "02:22:59 |     force_fp16_tokens: True\n",
            "02:22:59 |     fp16: True\n",
            "02:22:59 |     fp16_impl: safe\n",
            "02:22:59 |     gpu: 0\n",
            "02:22:59 |     gradient_clip: 10.0\n",
            "02:22:59 |     hide_labels: False\n",
            "02:22:59 |     history_add_global_end_token: None\n",
            "02:22:59 |     history_reversed: False\n",
            "02:22:59 |     history_size: -1\n",
            "02:22:59 |     image_cropsize: 224\n",
            "02:22:59 |     image_mode: raw\n",
            "02:22:59 |     image_size: 256\n",
            "02:22:59 |     inference: beam\n",
            "02:22:59 |     init_model: None\n",
            "02:22:59 |     init_opt: None\n",
            "02:22:59 |     interactive_mode: True\n",
            "02:22:59 |     interactive_task: True\n",
            "02:22:59 |     invsqrt_lr_decay_gamma: -1\n",
            "02:22:59 |     label_truncate: 128\n",
            "02:22:59 |     learn_positional_embeddings: True\n",
            "02:22:59 |     learningrate: 0.0005\n",
            "02:22:59 |     local_human_candidates_file: None\n",
            "02:22:59 |     log_every_n_secs: 30.0\n",
            "02:22:59 |     log_keep_fields: all\n",
            "02:22:59 |     loglevel: info\n",
            "02:22:59 |     lr_scheduler: invsqrt\n",
            "02:22:59 |     lr_scheduler_decay: 0.5\n",
            "02:22:59 |     lr_scheduler_patience: 3\n",
            "02:22:59 |     max_lr_steps: -1\n",
            "02:22:59 |     max_train_time: -1\n",
            "02:22:59 |     metrics: default\n",
            "02:22:59 |     model: transformer/generator\n",
            "02:22:59 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:22:59 |     model_parallel: False\n",
            "02:22:59 |     momentum: 0\n",
            "02:22:59 |     multitask_weights: [1]\n",
            "02:22:59 |     n_decoder_layers: -1\n",
            "02:22:59 |     n_encoder_layers: -1\n",
            "02:22:59 |     n_heads: 16\n",
            "02:22:59 |     n_layers: 8\n",
            "02:22:59 |     n_positions: 512\n",
            "02:22:59 |     n_segments: 0\n",
            "02:22:59 |     nesterov: True\n",
            "02:22:59 |     no_cuda: False\n",
            "02:22:59 |     num_epochs: 5.0\n",
            "02:22:59 |     numthreads: 1\n",
            "02:22:59 |     numworkers: 4\n",
            "02:22:59 |     nus: [0.7]\n",
            "02:22:59 |     optimizer: fused_adam\n",
            "02:22:59 |     outfile: \n",
            "02:22:59 |     output_scaling: 1.0\n",
            "02:22:59 |     override: \"{'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model'}\"\n",
            "02:22:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:22:59 |     person_tokens: False\n",
            "02:22:59 |     port: 61337\n",
            "02:22:59 |     pytorch_context_length: -1\n",
            "02:22:59 |     pytorch_datapath: None\n",
            "02:22:59 |     pytorch_include_labels: True\n",
            "02:22:59 |     pytorch_preprocess: False\n",
            "02:22:59 |     pytorch_teacher_batch_sort: False\n",
            "02:22:59 |     pytorch_teacher_dataset: None\n",
            "02:22:59 |     pytorch_teacher_task: None\n",
            "02:22:59 |     rank_candidates: False\n",
            "02:22:59 |     relu_dropout: 0.0\n",
            "02:22:59 |     save_after_valid: True\n",
            "02:22:59 |     save_every_n_secs: -1\n",
            "02:22:59 |     save_format: conversations\n",
            "02:22:59 |     share_word_embeddings: True\n",
            "02:22:59 |     short_final_eval: True\n",
            "02:22:59 |     show_advanced_args: False\n",
            "02:22:59 |     shuffle: False\n",
            "02:22:59 |     single_turn: False\n",
            "02:22:59 |     skip_generation: False\n",
            "02:22:59 |     special_tok_lst: None\n",
            "02:22:59 |     split_lines: False\n",
            "02:22:59 |     starttime: Mar19_02-22\n",
            "02:22:59 |     task: internal:new_reddit:presorted\n",
            "02:22:59 |     temperature: 1.0\n",
            "02:22:59 |     tensorboard_log: False\n",
            "02:22:59 |     text_truncate: 512\n",
            "02:22:59 |     topk: 10\n",
            "02:22:59 |     topp: 0.9\n",
            "02:22:59 |     truncate: -1\n",
            "02:22:59 |     update_freq: 1\n",
            "02:22:59 |     use_reply: label\n",
            "02:22:59 |     validation_cutoff: 1.0\n",
            "02:22:59 |     validation_every_n_epochs: -1\n",
            "02:22:59 |     validation_every_n_secs: 1800.0\n",
            "02:22:59 |     validation_max_exs: 9920\n",
            "02:22:59 |     validation_metric: ppl\n",
            "02:22:59 |     validation_metric_mode: min\n",
            "02:22:59 |     validation_patience: 0\n",
            "02:22:59 |     validation_share_agent: False\n",
            "02:22:59 |     variant: xlm\n",
            "02:22:59 |     verbose: False\n",
            "02:22:59 |     warmup_rate: 0.0001\n",
            "02:22:59 |     warmup_updates: 20000\n",
            "02:22:59 |     weight_decay: 0.01\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "02:22:59 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m Hello\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1m+ / u / dogetipbot 10 doge\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what do you mean\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mit ' s a bot that ' s been around for a while .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what bot\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi don ' t know , but i ' ve seen it before .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m when did you see it\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m not sure , but it was a while ago .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m cool, that's fine\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' ll see if i can find it .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m where are you now\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1min the middle of the night , i think .\u001b[0;0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-70576b1c7866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m Interactive.main(\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zoo:tutorial_transformer_generator/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36minteractive\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Show some example dialogs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_parleys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# chat was reset with [DONE], [EXIT] or EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/tasks/interactive/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/local_human/local_human.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreply_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter Your Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6S86h6kTFz0"
      },
      "source": [
        "**for command line**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA2ZIzoPTL9G"
      },
      "source": [
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgVXOFmtTX6q"
      },
      "source": [
        "# Quick view the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnNu5oHHTUkA",
        "outputId": "710f83a4-a34d-4077-88e7-937c90416308"
      },
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:26:18 | Opt:\n",
            "02:26:18 |     allow_missing_init_opts: False\n",
            "02:26:18 |     batchsize: 1\n",
            "02:26:18 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:26:18 |     datatype: train:ordered\n",
            "02:26:18 |     dict_class: None\n",
            "02:26:18 |     display_add_fields: \n",
            "02:26:18 |     download_path: None\n",
            "02:26:18 |     dynamic_batching: None\n",
            "02:26:18 |     hide_labels: False\n",
            "02:26:18 |     ignore_agent_reply: True\n",
            "02:26:18 |     image_cropsize: 224\n",
            "02:26:18 |     image_mode: raw\n",
            "02:26:18 |     image_size: 256\n",
            "02:26:18 |     init_model: None\n",
            "02:26:18 |     init_opt: None\n",
            "02:26:18 |     loglevel: info\n",
            "02:26:18 |     max_display_len: 1000\n",
            "02:26:18 |     model: None\n",
            "02:26:18 |     model_file: None\n",
            "02:26:18 |     multitask_weights: [1]\n",
            "02:26:18 |     mutators: None\n",
            "02:26:18 |     num_examples: 10\n",
            "02:26:18 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 10}\"\n",
            "02:26:18 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:26:18 |     remove_political_convos: False\n",
            "02:26:18 |     starttime: Mar19_02-26\n",
            "02:26:18 |     task: empathetic_dialogues\n",
            "02:26:18 |     train_experiencer_only: False\n",
            "02:26:18 |     verbose: False\n",
            "02:26:18 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues]\n",
            "02:26:18 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:02<00:00, 11.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:26:22 | \u001b[33mparlai.tasks.empathetic_dialogues.agents.DefaultTeacher' is outputting dicts instead of messages. If this is a teacher that is part of ParlAI, please file an issue on GitHub. If it is your own teacher, please return a Message object instead.\u001b[0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0m it feels like hitting to blank wall when i see the darkness\u001b[0;0m\n",
            "   \u001b[1;94mOh ya? I don't really see how\u001b[0;0m\n",
            "\u001b[0mdont you feel so.. its a wonder \u001b[0;0m\n",
            "   \u001b[1;94mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "\u001b[0m i virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "   \u001b[1;94mWait what are sweatings\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mOh ya? I don't really see how\u001b[0;0m\n",
            "   \u001b[1;94mdont you feel so.. its a wonder \u001b[0;0m\n",
            "\u001b[0mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "   \u001b[1;94m i virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "02:26:22 | loaded 39057 episodes with a total of 64636 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNmZ1xGWTwTm",
        "outputId": "bfdb249d-ebb0-4590-c459-063a7e8ce2c9"
      },
      "source": [
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:27:54 | Opt:\n",
            "02:27:54 |     allow_missing_init_opts: False\n",
            "02:27:54 |     batchsize: 1\n",
            "02:27:54 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:27:54 |     datatype: valid\n",
            "02:27:54 |     dict_class: None\n",
            "02:27:54 |     display_add_fields: \n",
            "02:27:54 |     download_path: None\n",
            "02:27:54 |     dynamic_batching: None\n",
            "02:27:54 |     hide_labels: False\n",
            "02:27:54 |     ignore_agent_reply: True\n",
            "02:27:54 |     image_cropsize: 224\n",
            "02:27:54 |     image_mode: raw\n",
            "02:27:54 |     image_size: 256\n",
            "02:27:54 |     init_model: None\n",
            "02:27:54 |     init_opt: None\n",
            "02:27:54 |     loglevel: info\n",
            "02:27:54 |     max_display_len: 1000\n",
            "02:27:54 |     model: None\n",
            "02:27:54 |     model_file: None\n",
            "02:27:54 |     multitask_weights: [1]\n",
            "02:27:54 |     mutators: None\n",
            "02:27:54 |     num_examples: 3\n",
            "02:27:54 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
            "02:27:54 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:27:54 |     remove_political_convos: False\n",
            "02:27:54 |     starttime: Mar19_02-27\n",
            "02:27:54 |     task: empathetic_dialogues\n",
            "02:27:54 |     train_experiencer_only: False\n",
            "02:27:54 |     verbose: False\n",
            "02:27:54 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "02:27:54 | loaded 2769 episodes with a total of 5738 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Jdcb77UA8r"
      },
      "source": [
        "**for command line**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ZoX3v-UEZw"
      },
      "source": [
        "those perform the same\n",
        "\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v8ax8mZUQMw"
      },
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGQUa8NoT96Z",
        "outputId": "e43047cd-e112-48f8-8e02-98f828dfdb04"
      },
      "source": [
        "%%time\n",
        "# make a new directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # filename with path\n",
        "    model_file='from_scratch_model/model',\n",
        "    # task name\n",
        "    task='empathetic_dialogues',\n",
        "    # training time, and batchsize\n",
        "    max_train_time=25 * 60,\n",
        "    batchsize=120,\n",
        "    model='seq2seq',\n",
        "    embedding_type='fasttext',\n",
        "    attention='dot',\n",
        "    lookuptable='all',\n",
        "    truncate=640,\n",
        ")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:33:13 | building dictionary first...\n",
            "02:33:13 | Opt:\n",
            "02:33:13 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:33:13 |     adam_eps: 1e-08\n",
            "02:33:13 |     add_p1_after_newln: False\n",
            "02:33:13 |     aggregate_micro: False\n",
            "02:33:13 |     allow_missing_init_opts: False\n",
            "02:33:13 |     attention: dot\n",
            "02:33:13 |     attention_length: 48\n",
            "02:33:13 |     attention_time: post\n",
            "02:33:13 |     batchsize: 1\n",
            "02:33:13 |     beam_block_full_context: True\n",
            "02:33:13 |     beam_block_list_filename: None\n",
            "02:33:13 |     beam_block_ngram: -1\n",
            "02:33:13 |     beam_context_block_ngram: -1\n",
            "02:33:13 |     beam_delay: 30\n",
            "02:33:13 |     beam_length_penalty: 0.65\n",
            "02:33:13 |     beam_min_length: 1\n",
            "02:33:13 |     beam_size: 1\n",
            "02:33:13 |     betas: '(0.9, 0.999)'\n",
            "02:33:13 |     bidirectional: False\n",
            "02:33:13 |     bpe_add_prefix_space: None\n",
            "02:33:13 |     bpe_debug: False\n",
            "02:33:13 |     bpe_dropout: None\n",
            "02:33:13 |     bpe_merge: None\n",
            "02:33:13 |     bpe_vocab: None\n",
            "02:33:13 |     compute_tokenized_bleu: False\n",
            "02:33:13 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:33:13 |     datatype: train\n",
            "02:33:13 |     decoder: same\n",
            "02:33:13 |     delimiter: '\\n'\n",
            "02:33:13 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:33:13 |     dict_endtoken: __end__\n",
            "02:33:13 |     dict_file: from_scratch_model/model.dict\n",
            "02:33:13 |     dict_include_test: False\n",
            "02:33:13 |     dict_include_valid: False\n",
            "02:33:13 |     dict_initpath: None\n",
            "02:33:13 |     dict_language: english\n",
            "02:33:13 |     dict_loaded: False\n",
            "02:33:13 |     dict_lower: False\n",
            "02:33:13 |     dict_max_ngram_size: -1\n",
            "02:33:13 |     dict_maxexs: -1\n",
            "02:33:13 |     dict_maxtokens: -1\n",
            "02:33:13 |     dict_minfreq: 0\n",
            "02:33:13 |     dict_nulltoken: __null__\n",
            "02:33:13 |     dict_starttoken: __start__\n",
            "02:33:13 |     dict_textfields: text,labels\n",
            "02:33:13 |     dict_tokenizer: re\n",
            "02:33:13 |     dict_unktoken: __unk__\n",
            "02:33:13 |     display_examples: False\n",
            "02:33:13 |     download_path: None\n",
            "02:33:13 |     dropout: 0.1\n",
            "02:33:13 |     dynamic_batching: None\n",
            "02:33:13 |     embedding_projection: random\n",
            "02:33:13 |     embedding_type: fasttext\n",
            "02:33:13 |     embeddingsize: 128\n",
            "02:33:13 |     eval_batchsize: None\n",
            "02:33:13 |     eval_dynamic_batching: None\n",
            "02:33:13 |     evaltask: None\n",
            "02:33:13 |     force_fp16_tokens: False\n",
            "02:33:13 |     fp16: False\n",
            "02:33:13 |     fp16_impl: safe\n",
            "02:33:13 |     gpu: -1\n",
            "02:33:13 |     gradient_clip: 0.1\n",
            "02:33:13 |     hiddensize: 128\n",
            "02:33:13 |     hide_labels: False\n",
            "02:33:13 |     history_add_global_end_token: None\n",
            "02:33:13 |     history_reversed: False\n",
            "02:33:13 |     history_size: -1\n",
            "02:33:13 |     image_cropsize: 224\n",
            "02:33:13 |     image_mode: no_image_model\n",
            "02:33:13 |     image_size: 256\n",
            "02:33:13 |     inference: greedy\n",
            "02:33:13 |     init_model: None\n",
            "02:33:13 |     init_opt: None\n",
            "02:33:13 |     input_dropout: 0.0\n",
            "02:33:13 |     interactive_mode: False\n",
            "02:33:13 |     invsqrt_lr_decay_gamma: -1\n",
            "02:33:13 |     label_truncate: None\n",
            "02:33:13 |     learningrate: 1\n",
            "02:33:13 |     load_from_checkpoint: True\n",
            "02:33:13 |     log_every_n_secs: 10\n",
            "02:33:13 |     loglevel: info\n",
            "02:33:13 |     lookuptable: all\n",
            "02:33:13 |     lr_scheduler: reduceonplateau\n",
            "02:33:13 |     lr_scheduler_decay: 0.5\n",
            "02:33:13 |     lr_scheduler_patience: 3\n",
            "02:33:13 |     max_lr_steps: -1\n",
            "02:33:13 |     max_train_time: 1500.0\n",
            "02:33:13 |     metrics: default\n",
            "02:33:13 |     model: seq2seq\n",
            "02:33:13 |     model_file: from_scratch_model/model\n",
            "02:33:13 |     momentum: 0\n",
            "02:33:13 |     multitask_weights: [1]\n",
            "02:33:13 |     mutators: None\n",
            "02:33:13 |     nesterov: True\n",
            "02:33:13 |     no_cuda: False\n",
            "02:33:13 |     num_epochs: -1\n",
            "02:33:13 |     numlayers: 2\n",
            "02:33:13 |     numsoftmax: 1\n",
            "02:33:13 |     nus: (0.7,)\n",
            "02:33:13 |     optimizer: sgd\n",
            "02:33:13 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 1500.0, 'batchsize': 120, 'model': 'seq2seq', 'embedding_type': 'fasttext', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 640}\"\n",
            "02:33:13 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:33:13 |     person_tokens: False\n",
            "02:33:13 |     rank_candidates: False\n",
            "02:33:13 |     remove_political_convos: False\n",
            "02:33:13 |     rnn_class: lstm\n",
            "02:33:13 |     save_after_valid: False\n",
            "02:33:13 |     save_every_n_secs: -1\n",
            "02:33:13 |     short_final_eval: False\n",
            "02:33:13 |     skip_generation: False\n",
            "02:33:13 |     special_tok_lst: None\n",
            "02:33:13 |     split_lines: False\n",
            "02:33:13 |     starttime: Mar19_02-33\n",
            "02:33:13 |     task: empathetic_dialogues\n",
            "02:33:13 |     temperature: 1.0\n",
            "02:33:13 |     tensorboard_log: False\n",
            "02:33:13 |     tensorboard_logdir: None\n",
            "02:33:13 |     text_truncate: None\n",
            "02:33:13 |     topk: 10\n",
            "02:33:13 |     topp: 0.9\n",
            "02:33:13 |     train_experiencer_only: False\n",
            "02:33:13 |     truncate: 640\n",
            "02:33:13 |     update_freq: 1\n",
            "02:33:13 |     use_reply: label\n",
            "02:33:13 |     validation_cutoff: 1.0\n",
            "02:33:13 |     validation_every_n_epochs: -1\n",
            "02:33:13 |     validation_every_n_secs: -1\n",
            "02:33:13 |     validation_max_exs: -1\n",
            "02:33:13 |     validation_metric: accuracy\n",
            "02:33:13 |     validation_metric_mode: None\n",
            "02:33:13 |     validation_patience: 10\n",
            "02:33:13 |     validation_share_agent: False\n",
            "02:33:13 |     verbose: False\n",
            "02:33:13 |     wandb_log: False\n",
            "02:33:13 |     wandb_name: None\n",
            "02:33:13 |     wandb_project: None\n",
            "02:33:13 |     warmup_rate: 0.0001\n",
            "02:33:13 |     warmup_updates: -1\n",
            "02:33:13 |     weight_decay: None\n",
            "02:33:13 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:02<00:00, 26.0kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:33:16 | Saving dictionary to from_scratch_model/model.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:33:16 | dictionary built with 22419 tokens in 0.0s\n",
            "02:33:16 | No model with opt yet at: from_scratch_model/model(.opt)\n",
            "02:33:16 | Using CUDA\n",
            "02:33:16 | loading dictionary from from_scratch_model/model.dict\n",
            "02:33:17 | num words = 22419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/data/models/fasttext_vectors/wiki.en.vec: 6.60GB [05:10, 21.2MB/s]                            \n",
            "  0%|          | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 2518782/2519370 [04:10<00:00, 10940.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:46:23 | Initialized embeddings for 16270 tokens (72.6%) from fasttext.\n",
            "02:46:24 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "02:46:24 | Opt:\n",
            "02:46:24 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:46:24 |     adam_eps: 1e-08\n",
            "02:46:24 |     add_p1_after_newln: False\n",
            "02:46:24 |     aggregate_micro: False\n",
            "02:46:24 |     allow_missing_init_opts: False\n",
            "02:46:24 |     attention: dot\n",
            "02:46:24 |     attention_length: 48\n",
            "02:46:24 |     attention_time: post\n",
            "02:46:24 |     batchsize: 120\n",
            "02:46:24 |     beam_block_full_context: True\n",
            "02:46:24 |     beam_block_list_filename: None\n",
            "02:46:24 |     beam_block_ngram: -1\n",
            "02:46:24 |     beam_context_block_ngram: -1\n",
            "02:46:24 |     beam_delay: 30\n",
            "02:46:24 |     beam_length_penalty: 0.65\n",
            "02:46:24 |     beam_min_length: 1\n",
            "02:46:24 |     beam_size: 1\n",
            "02:46:24 |     betas: '(0.9, 0.999)'\n",
            "02:46:24 |     bidirectional: False\n",
            "02:46:24 |     bpe_add_prefix_space: None\n",
            "02:46:24 |     bpe_debug: False\n",
            "02:46:24 |     bpe_dropout: None\n",
            "02:46:24 |     bpe_merge: None\n",
            "02:46:24 |     bpe_vocab: None\n",
            "02:46:24 |     compute_tokenized_bleu: False\n",
            "02:46:24 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:46:24 |     datatype: train\n",
            "02:46:24 |     decoder: same\n",
            "02:46:24 |     delimiter: '\\n'\n",
            "02:46:24 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:46:24 |     dict_endtoken: __end__\n",
            "02:46:24 |     dict_file: from_scratch_model/model.dict\n",
            "02:46:24 |     dict_include_test: False\n",
            "02:46:24 |     dict_include_valid: False\n",
            "02:46:24 |     dict_initpath: None\n",
            "02:46:24 |     dict_language: english\n",
            "02:46:24 |     dict_loaded: True\n",
            "02:46:24 |     dict_lower: False\n",
            "02:46:24 |     dict_max_ngram_size: -1\n",
            "02:46:24 |     dict_maxexs: -1\n",
            "02:46:24 |     dict_maxtokens: -1\n",
            "02:46:24 |     dict_minfreq: 0\n",
            "02:46:24 |     dict_nulltoken: __null__\n",
            "02:46:24 |     dict_starttoken: __start__\n",
            "02:46:24 |     dict_textfields: text,labels\n",
            "02:46:24 |     dict_tokenizer: re\n",
            "02:46:24 |     dict_unktoken: __unk__\n",
            "02:46:24 |     display_examples: False\n",
            "02:46:24 |     download_path: None\n",
            "02:46:24 |     dropout: 0.1\n",
            "02:46:24 |     dynamic_batching: None\n",
            "02:46:24 |     embedding_projection: random\n",
            "02:46:24 |     embedding_type: fasttext\n",
            "02:46:24 |     embeddingsize: 128\n",
            "02:46:24 |     eval_batchsize: None\n",
            "02:46:24 |     eval_dynamic_batching: None\n",
            "02:46:24 |     evaltask: None\n",
            "02:46:24 |     force_fp16_tokens: False\n",
            "02:46:24 |     fp16: False\n",
            "02:46:24 |     fp16_impl: safe\n",
            "02:46:24 |     gpu: -1\n",
            "02:46:24 |     gradient_clip: 0.1\n",
            "02:46:24 |     hiddensize: 128\n",
            "02:46:24 |     hide_labels: False\n",
            "02:46:24 |     history_add_global_end_token: None\n",
            "02:46:24 |     history_reversed: False\n",
            "02:46:24 |     history_size: -1\n",
            "02:46:24 |     image_cropsize: 224\n",
            "02:46:24 |     image_mode: raw\n",
            "02:46:24 |     image_size: 256\n",
            "02:46:24 |     inference: greedy\n",
            "02:46:24 |     init_model: None\n",
            "02:46:24 |     init_opt: None\n",
            "02:46:24 |     input_dropout: 0.0\n",
            "02:46:24 |     interactive_mode: False\n",
            "02:46:24 |     invsqrt_lr_decay_gamma: -1\n",
            "02:46:24 |     label_truncate: None\n",
            "02:46:24 |     learningrate: 1\n",
            "02:46:24 |     load_from_checkpoint: True\n",
            "02:46:24 |     log_every_n_secs: 10\n",
            "02:46:24 |     loglevel: info\n",
            "02:46:24 |     lookuptable: all\n",
            "02:46:24 |     lr_scheduler: reduceonplateau\n",
            "02:46:24 |     lr_scheduler_decay: 0.5\n",
            "02:46:24 |     lr_scheduler_patience: 3\n",
            "02:46:24 |     max_lr_steps: -1\n",
            "02:46:24 |     max_train_time: 1500.0\n",
            "02:46:24 |     metrics: default\n",
            "02:46:24 |     model: seq2seq\n",
            "02:46:24 |     model_file: from_scratch_model/model\n",
            "02:46:24 |     momentum: 0\n",
            "02:46:24 |     multitask_weights: [1]\n",
            "02:46:24 |     mutators: None\n",
            "02:46:24 |     nesterov: True\n",
            "02:46:24 |     no_cuda: False\n",
            "02:46:24 |     num_epochs: -1\n",
            "02:46:24 |     numlayers: 2\n",
            "02:46:24 |     numsoftmax: 1\n",
            "02:46:24 |     nus: (0.7,)\n",
            "02:46:24 |     optimizer: sgd\n",
            "02:46:24 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 1500.0, 'batchsize': 120, 'model': 'seq2seq', 'embedding_type': 'fasttext', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 640}\"\n",
            "02:46:24 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:46:24 |     person_tokens: False\n",
            "02:46:24 |     rank_candidates: False\n",
            "02:46:24 |     remove_political_convos: False\n",
            "02:46:24 |     rnn_class: lstm\n",
            "02:46:24 |     save_after_valid: False\n",
            "02:46:24 |     save_every_n_secs: -1\n",
            "02:46:24 |     short_final_eval: False\n",
            "02:46:24 |     skip_generation: False\n",
            "02:46:24 |     special_tok_lst: None\n",
            "02:46:24 |     split_lines: False\n",
            "02:46:24 |     starttime: Mar19_02-33\n",
            "02:46:24 |     task: empathetic_dialogues\n",
            "02:46:24 |     temperature: 1.0\n",
            "02:46:24 |     tensorboard_log: False\n",
            "02:46:24 |     tensorboard_logdir: None\n",
            "02:46:24 |     text_truncate: None\n",
            "02:46:24 |     topk: 10\n",
            "02:46:24 |     topp: 0.9\n",
            "02:46:24 |     train_experiencer_only: False\n",
            "02:46:24 |     truncate: 640\n",
            "02:46:24 |     update_freq: 1\n",
            "02:46:24 |     use_reply: label\n",
            "02:46:24 |     validation_cutoff: 1.0\n",
            "02:46:24 |     validation_every_n_epochs: -1\n",
            "02:46:24 |     validation_every_n_secs: -1\n",
            "02:46:24 |     validation_max_exs: -1\n",
            "02:46:24 |     validation_metric: accuracy\n",
            "02:46:24 |     validation_metric_mode: None\n",
            "02:46:24 |     validation_patience: 10\n",
            "02:46:24 |     validation_share_agent: False\n",
            "02:46:24 |     verbose: False\n",
            "02:46:24 |     wandb_log: False\n",
            "02:46:24 |     wandb_name: None\n",
            "02:46:24 |     wandb_project: None\n",
            "02:46:24 |     warmup_rate: 0.0001\n",
            "02:46:24 |     warmup_updates: -1\n",
            "02:46:24 |     weight_decay: None\n",
            "02:46:25 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "02:46:26 | training...\n",
            "02:46:36 | time:10s total_exs:5040 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3739 15584 500.1 5040  .2313    .3282 9.658   1  2031  8464 15646     .03945                   42 5770 24048 4.168\n",
            "\n",
            "02:46:46 | time:20s total_exs:10560 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3704 16756 542.8 5520  .3211    .3946 9.202   1  1954  8839 9922      .0406                   88 5658 25595 4.524\n",
            "\n",
            "02:46:56 | time:31s total_exs:16440 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3726 18064 581.8 5880  .3993    .2967 8.861   1  1968  9542 7049     .03993                  137 5694 27607 4.849\n",
            "\n",
            "02:47:06 | time:41s total_exs:22080 epochs:0.34\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3685 17343 564.7 5640  .4296    .3684  8.54   1  1965  9246 5117     .05391                  184 5650 26589 4.707\n",
            "\n",
            "02:47:17 | time:51s total_exs:27600 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3752 16972 542.8 5520  .4187    .3003 8.336   1  1983  8972 4170     .06566                  230 5735 25945 4.524\n",
            "\n",
            "02:47:27 | time:61s total_exs:33600 epochs:0.52\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3750 18688   598 6000  .4039    .2656 8.147   1  1971  9822 3454     .07109                  280 5721 28510 4.984\n",
            "\n",
            "02:47:37 | time:71s total_exs:38760 epochs:0.60\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3693 15904 516.7 5160  .3980    .3502 7.984   1  1994  8588 2935     .07121                  323 5688 24492 4.307\n",
            "\n",
            "02:47:47 | time:81s total_exs:44640 epochs:0.69\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3701 17920 581.1 5880  .3863    .3930 7.817   1  1943  9408 2484     .07066                  372 5644 27329 4.843\n",
            "\n",
            "02:47:57 | time:91s total_exs:50880 epochs:0.79\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3609 18632 619.4 6240  .3891    .2819 7.648   1  1925  9936 2096     .07185                  424 5534 28568 5.163\n",
            "\n",
            "02:48:07 | time:101s total_exs:56520 epochs:0.87\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3704 17377   563 5640  .3840    .4229 7.547   1  1981  9296 1895     .07335                  471 5685 26673 4.692\n",
            "\n",
            "02:48:17 | time:111s total_exs:62040 epochs:0.96\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3714 16692 539.4 5520  .3896    .2999 7.454   1  1955  8790 1727     .07008                  517 5669 25482 4.495\n",
            "\n",
            "02:48:27 | time:122s total_exs:67680 epochs:1.05\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3730 17476 562.1 5640  .3811    .3252  7.37   1  1941  9093 1588     .07092                  564 5671 26569 4.685\n",
            "\n",
            "02:48:37 | time:132s total_exs:73440 epochs:1.14\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3644 17421 573.6 5760  .3573    .3069 7.312   1  1977  9450 1498     .07539                  612 5621 26872 4.781\n",
            "\n",
            "02:48:47 | time:142s total_exs:78960 epochs:1.22\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3686 16943 551.6 5520  .3655    .2999 7.232   1  1983  9114 1383     .07643                  658 5668 26057 4.597\n",
            "\n",
            "02:48:58 | time:152s total_exs:84960 epochs:1.31\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3637 17998 593.8 6000  .3562    .2747 7.149   1  1937  9583 1273     .08327                  708 5574 27581 4.949\n",
            "\n",
            "02:49:08 | time:162s total_exs:90840 epochs:1.41\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3684 17897 582.9 5880  .3366    .3683 7.089   1  1950  9473 1199      .0860                  757 5634 27370 4.858\n",
            "\n",
            "02:49:18 | time:172s total_exs:96600 epochs:1.49\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3657 17424 571.6 5760  .3115    .3929 7.001   1  1928  9183 1098     .09671                  805 5585 26607 4.764\n",
            "\n",
            "02:49:28 | time:182s total_exs:102120 epochs:1.58\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3682 16911 551.1 5520  .3106    .2965 6.992   1  1991  9145 1088      .1024                  851 5673 26057 4.593\n",
            "\n",
            "02:49:38 | time:192s total_exs:107640 epochs:1.67\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3637 16508 544.7 5520  .2921    .4240 6.865   1  1975  8964  958      .1096                  897 5611 25472 4.54\n",
            "\n",
            "02:49:48 | time:202s total_exs:113160 epochs:1.75\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3647 16737 550.7 5520  .2696    .4226 6.823   1  1963  9006 918.5      .1132                  943 5610 25743 4.589\n",
            "\n",
            "02:49:58 | time:212s total_exs:119040 epochs:1.84\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3676 17781 580.4 5880  .2778    .2676  6.73   1  1946  9412  837      .1160                  992 5623 27194 4.837\n",
            "\n",
            "02:50:08 | time:223s total_exs:124680 epochs:1.93\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3677 17158 559.9 5640  .2632    .3608  6.69   1  1946  9082  804      .1179                 1039 5624 26240 4.666\n",
            "\n",
            "02:50:18 | time:233s total_exs:130080 epochs:2.01\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3714 16663 538.3 5400  .2770    .4222 6.644   1  1967  8823  768      .1174                 1084 5681 25486 4.487\n",
            "\n",
            "02:50:28 | time:243s total_exs:135960 epochs:2.10\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3644 17672   582 5880  .2731    .3601 6.585   1  1962  9516 724.1      .1210                 1133 5606 27189 4.85\n",
            "\n",
            "02:50:39 | time:253s total_exs:141600 epochs:2.19\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3617 16940   562 5640  .2790    .3945 6.535   1  1959  9174 689.2      .1250                 1180 5576 26114 4.684\n",
            "\n",
            "02:50:49 | time:263s total_exs:147360 epochs:2.28\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3660 17399 570.5 5760  .2771    .3499 6.493   1  1950  9271 660.7      .1262                 1228 5609 26669 4.755\n",
            "\n",
            "02:50:59 | time:273s total_exs:153000 epochs:2.37\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3658 17120 561.6 5640  .2769    .2819 6.467   1  1958  9162 643.3      .1261                 1275 5615 26282 4.681\n",
            "\n",
            "02:51:09 | time:283s total_exs:158520 epochs:2.45\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3727 16953 545.8 5520  .2740    .3387 6.408   1  1973  8973 606.7      .1295                 1321 5700 25926 4.549\n",
            "\n",
            "02:51:19 | time:293s total_exs:164280 epochs:2.54\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3692 17456 567.3 5760  .2876    .3017 6.367   1  1970  9314 582.6      .1307                 1369 5662 26770 4.728\n",
            "\n",
            "02:51:29 | time:303s total_exs:169560 epochs:2.62\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3664 16083 526.7 5280  .2736    .3929 6.369   1  1995  8756 583.8      .1286                 1413 5659 24840 4.39\n",
            "\n",
            "02:51:39 | time:314s total_exs:175680 epochs:2.72\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3662 18316 600.1 6120  .2631    .2454 6.324   1  1933  9668 558.1      .1305                 1464 5595 27984 5.002\n",
            "\n",
            "02:51:50 | time:324s total_exs:181560 epochs:2.81\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3692 17891 581.5 5880  .2646    .2743 6.278   1  1978  9584 532.5      .1319                 1513 5670 27475 4.846\n",
            "\n",
            "02:52:00 | time:334s total_exs:187200 epochs:2.90\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3703 17083 553.6 5640  .2711    .3682 6.259   1  1976  9118 522.6      .1305                 1560 5679 26202 4.614\n",
            "\n",
            "02:52:10 | time:344s total_exs:192720 epochs:2.98\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3610 16596 551.6 5520  .2611    .3745 6.202   1  1972  9063 493.5      .1333                 1606 5582 25659 4.597\n",
            "\n",
            "02:52:20 | time:354s total_exs:198600 epochs:3.07\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3659 17936 588.2 5880  .2849    .2795 6.192   1  1977  9694 488.7      .1352                 1655 5636 27629 4.902\n",
            "\n",
            "02:52:30 | time:364s total_exs:204360 epochs:3.16\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3683 17090 556.9 5760  .2803    .3682 6.138   1  1947  9035 462.9      .1380                 1703 5630 26125 4.641\n",
            "\n",
            "02:52:41 | time:375s total_exs:209760 epochs:3.25\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3664 15997 523.9 5400  .2907    .3929 6.124   1  1959  8552 456.8      .1385                 1748 5623 24549 4.367\n",
            "\n",
            "02:52:51 | time:385s total_exs:215280 epochs:3.33\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3780 17179 545.3 5520  .2836    .3040 6.114   1  1982  9005 452.3      .1370                 1794 5762 26185 4.545\n",
            "\n",
            "02:53:01 | time:395s total_exs:220800 epochs:3.42\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3697 16682 541.5 5520  .2683    .3950 6.069   1  1944  8770 432.2      .1401                 1840 5640 25452 4.513\n",
            "\n",
            "02:53:11 | time:405s total_exs:226200 epochs:3.50\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3733 16657 535.5 5400  .2872    .4231 6.068   1  1951  8705 431.6      .1438                 1885 5683 25362 4.463\n",
            "\n",
            "02:53:21 | time:415s total_exs:232320 epochs:3.59\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3690 18814 611.7 6120  .2938    .2963 6.013   1  1922  9801 408.6      .1481                 1936 5613 28615 5.099\n",
            "\n",
            "02:53:31 | time:426s total_exs:237960 epochs:3.68\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3728 17184 553.1 5640  .2609    .4234 6.019   1  1978  9119  411      .1502                 1983 5706 26302 4.61\n",
            "\n",
            "02:53:41 | time:436s total_exs:243120 epochs:3.76\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3692 15736 511.4 5160  .2716    .3952 5.996   1  2000  8525 401.8      .1536                 2026 5692 24261 4.262\n",
            "\n",
            "02:53:52 | time:446s total_exs:248880 epochs:3.85\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3717 17648 569.7 5760  .2853    .3618 5.976   1  1974  9372 393.8      .1560                 2074 5691 27021 4.749\n",
            "\n",
            "02:54:02 | time:456s total_exs:254640 epochs:3.94\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3686 17664 575.1 5760  .2825    .3267  5.96   1  1938  9286 387.6      .1551                 2122 5623 26950 4.793\n",
            "\n",
            "02:54:12 | time:466s total_exs:260520 epochs:4.03\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3672 17661 577.1 5880  .2959    .3101 5.924   1  1945  9354 373.8      .1572                 2171 5617 27015 4.81\n",
            "\n",
            "02:54:22 | time:476s total_exs:266160 epochs:4.12\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3689 17087 555.8 5640  .2925    .3276 5.936   1  1991  9222 378.6      .1571                 2218 5680 26309 4.632\n",
            "\n",
            "02:54:32 | time:486s total_exs:272160 epochs:4.21\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3676 18317 597.9 6000  .3027    .3747 5.882   1  1951  9720 358.6      .1614                 2268 5626 28037 4.983\n",
            "\n",
            "02:54:42 | time:496s total_exs:277920 epochs:4.30\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3696 17504 568.3 5760  .3020    .3609 5.863   1  1960  9284 351.7      .1637                 2316 5656 26788 4.737\n",
            "\n",
            "02:54:52 | time:507s total_exs:283440 epochs:4.39\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3688 16829 547.6 5520  .3086    .4229 5.852   1  1940  8851 348.1      .1638                 2362 5627 25680 4.564\n",
            "\n",
            "02:55:02 | time:517s total_exs:289680 epochs:4.48\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3688 19049 619.9 6240  .3089    .2820 5.828   1  1969 10170 339.7      .1667                 2414 5656 29218 5.166\n",
            "\n",
            "02:55:13 | time:527s total_exs:295560 epochs:4.57\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3716 17982 580.7 5880  .3038    .3241   5.8   1  1971  9539 330.4      .1686                 2463 5687 27522 4.84\n",
            "\n",
            "02:55:23 | time:537s total_exs:301320 epochs:4.66\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3654 17521 575.4 5760  .3101    .3252 5.801   1  1944  9319 330.6      .1677                 2511 5598 26840 4.795\n",
            "\n",
            "02:55:33 | time:547s total_exs:307440 epochs:4.76\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3685 18593 605.5 6120  .3110    .3682 5.758   1  1947  9826 316.8      .1669                 2562 5632 28419 5.046\n",
            "\n",
            "02:55:43 | time:557s total_exs:312600 epochs:4.84\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3691 15703 510.5 5160  .2985    .4218 5.768   1  1967  8367  320      .1699                 2605 5658 24070 4.254\n",
            "\n",
            "02:55:53 | time:567s total_exs:318360 epochs:4.93\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3698 17688 573.9 5760  .3086    .3748 5.735   1  1942  9288 309.7      .1732                 2653 5640 26978 4.784\n",
            "\n",
            "02:56:03 | time:577s total_exs:324000 epochs:5.01\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3600 16563   552 5640  .3170    .3102 5.748   1  1969  9059 313.6      .1697                 2700 5569 25623 4.601\n",
            "\n",
            "02:56:13 | time:588s total_exs:329880 epochs:5.10\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3649 17675 581.3 5880  .3191    .2999 5.728   1  1942  9410 307.2      .1717                 2749 5591 27085 4.845\n",
            "\n",
            "02:56:24 | time:598s total_exs:335640 epochs:5.19\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3685 17513 570.4 5760  .3192    .3744 5.717   1  1958  9308 303.9      .1717                 2797 5643 26821 4.753\n",
            "\n",
            "02:56:34 | time:608s total_exs:341280 epochs:5.28\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3639 17025 561.4 5640  .3133    .3501 5.701   1  1955  9146 299.3      .1730                 2844 5594 26171 4.679\n",
            "\n",
            "02:56:44 | time:618s total_exs:346440 epochs:5.36\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3674 15808 516.4 5160  .3099    .4232 5.669   1  1980  8519 289.9      .1730                 2887 5653 24327 4.303\n",
            "\n",
            "02:56:54 | time:628s total_exs:352200 epochs:5.45\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3727 17819 573.8 5760  .3155    .4225 5.696   1  1959  9366 297.7      .1720                 2935 5686 27185 4.782\n",
            "\n",
            "02:57:04 | time:638s total_exs:357600 epochs:5.53\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3750 16843 538.9 5400  .3084    .3272  5.67   1  1979  8887  290      .1751                 2980 5729 25729 4.491\n",
            "\n",
            "02:57:14 | time:648s total_exs:363360 epochs:5.62\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3654 17423 572.2 5760  .3148    .4224  5.64   1  1958  9337 281.4      .1779                 3028 5612 26761 4.769\n",
            "\n",
            "02:57:24 | time:658s total_exs:369120 epochs:5.71\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3741 17643 565.8 5760  .3102    .4220 5.631   1  1953  9211 278.8      .1757                 3076 5694 26855 4.716\n",
            "\n",
            "02:57:34 | time:668s total_exs:374280 epochs:5.79\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3688 15699 510.8 5160  .3179    .4222 5.621   1  1993  8485 276.2      .1785                 3119 5682 24184 4.257\n",
            "\n",
            "02:57:44 | time:679s total_exs:380160 epochs:5.88\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3715 17987   581 5880  .3104    .2766 5.616   1  1969  9531 274.9      .1778                 3168 5684 27519 4.842\n",
            "\n",
            "02:57:54 | time:689s total_exs:385560 epochs:5.97\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3737 16794 539.3 5400  .3120    .3748 5.591   1  1970  8853  268      .1812                 3213 5706 25647 4.495\n",
            "\n",
            "02:58:04 | time:699s total_exs:391200 epochs:6.05\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3707 17268 558.9 5640  .3036    .4236 5.559   1  1963  9145 259.6      .1821                 3260 5670 26413 4.659\n",
            "\n",
            "02:58:15 | time:709s total_exs:396720 epochs:6.14\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3610 16401 545.1 5520  .3308    .3749 5.562   1  1968  8940 260.3      .1817                 3306 5578 25341 4.543\n",
            "\n",
            "02:58:25 | time:719s total_exs:402600 epochs:6.23\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3659 17827 584.7 5880  .3209    .3386 5.582   1  1963  9566 265.5      .1824                 3355 5622 27394 4.873\n",
            "\n",
            "02:58:35 | time:729s total_exs:408600 epochs:6.32\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3668 18246 596.9 6000  .3144    .2734 5.577   1  1952  9711 264.4      .1819                 3405 5620 27957 4.974\n",
            "\n",
            "02:58:45 | time:739s total_exs:414600 epochs:6.41\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3728 18515 595.9 6000  .3114    .3024 5.567   1  1970  9781 261.6      .1807                 3455 5698 28296 4.966\n",
            "\n",
            "02:58:55 | time:749s total_exs:419880 epochs:6.50\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3650 16052 527.8 5280  .3079    .3001 5.537   1  1992  8759 253.8      .1849                 3499 5641 24810 4.399\n",
            "\n",
            "02:59:05 | time:759s total_exs:425640 epochs:6.59\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3696 17564 570.3 5760  .2977    .3243 5.526   1  1945  9242 251.2      .1839                 3547 5641 26806 4.753\n",
            "\n",
            "02:59:15 | time:769s total_exs:431880 epochs:6.68\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3715 19211 620.5 6240  .3264    .2255 5.504   1  1975 10215 245.6      .1875                 3599 5691 29427 5.171\n",
            "\n",
            "02:59:25 | time:780s total_exs:437520 epochs:6.77\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3695 17205 558.8 5640  .3322    .3003 5.512   1  1980  9222 247.7      .1853                 3646 5675 26427 4.657\n",
            "\n",
            "02:59:35 | time:790s total_exs:443400 epochs:6.86\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3605 17644 587.2 5880  .3331    .3619 5.508   1  1960  9594 246.7      .1875                 3695 5565 27238 4.895\n",
            "\n",
            "02:59:45 | time:800s total_exs:449040 epochs:6.95\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3742 17396 557.8 5640  .3233    .4223 5.477   1  1955  9089 239.1      .1872                 3742 5697 26486 4.649\n",
            "\n",
            "02:59:56 | time:810s total_exs:454560 epochs:7.03\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3633 16431 542.8 5520  .3361    .4230 5.474   1  1954  8838 238.4      .1894                 3788 5587 25269 4.524\n",
            "\n",
            "03:00:06 | time:820s total_exs:460320 epochs:7.12\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3733 17892 575.1 5760  .3307    .3743 5.466   1  1956  9376 236.6      .1910                 3836 5690 27268 4.793\n",
            "\n",
            "03:00:16 | time:830s total_exs:465960 epochs:7.21\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3662 16832 551.5 5640  .3317    .3752 5.471   1  1990  9146 237.8      .1892                 3883 5653 25978 4.596\n",
            "\n",
            "03:00:26 | time:840s total_exs:471840 epochs:7.30\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3732 18041 580.1 5880  .3164    .3607  5.46   1  1952  9437  235      .1922                 3932 5684 27478 4.834\n",
            "\n",
            "03:00:36 | time:851s total_exs:477360 epochs:7.39\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3655 16688 547.9 5520  .3124    .3684 5.431   1  1954  8922 228.3      .1939                 3978 5609 25611 4.566\n",
            "\n",
            "03:00:46 | time:861s total_exs:483480 epochs:7.48\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3712 18794 607.5 6120  .3266    .3015 5.425   1  1943  9837 227.1      .1950                 4029 5655 28630 5.063\n",
            "\n",
            "03:00:56 | time:871s total_exs:489240 epochs:7.57\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3774 18132 576.5 5760  .3278    .2957 5.423   1  1958  9406 226.6      .1950                 4077 5732 27538 4.805\n",
            "\n",
            "03:01:06 | time:881s total_exs:494760 epochs:7.65\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3705 16964 549.4 5520  .3101    .3950 5.413   1  1937  8869 224.2      .1957                 4123 5642 25834 4.579\n",
            "\n",
            "03:01:17 | time:891s total_exs:500520 epochs:7.74\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3744 17734 568.4 5760  .3306    .3684 5.405   1  1956  9264 222.6      .1984                 4171 5700 26998 4.737\n",
            "\n",
            "03:01:27 | time:901s total_exs:506040 epochs:7.83\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3677 16809 548.5 5520  .3338    .3101 5.421   1  1968  8998  226      .1948                 4217 5646 25807 4.572\n",
            "\n",
            "03:01:37 | time:911s total_exs:512280 epochs:7.93\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3613 18681 620.4 6240  .3344    .2934 5.377   1  1958 10124 216.4      .1987                 4269 5572 28805 5.17\n",
            "\n",
            "03:01:47 | time:921s total_exs:518160 epochs:8.02\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3708 17943 580.7 5880  .3271    .3758 5.389   1  1960  9486  219      .2004                 4318 5668 27429 4.839\n",
            "\n",
            "03:01:57 | time:931s total_exs:524040 epochs:8.11\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3677 17717 578.2 5880  .3077    .3104  5.39   1  1969  9486 219.1      .2000                 4367 5645 27203 4.819\n",
            "\n",
            "03:02:07 | time:942s total_exs:530160 epochs:8.20\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3611 18154 603.3 6120  .3232    .3271 5.355   1  1929  9699 211.6      .2023                 4418 5540 27853 5.028\n",
            "\n",
            "03:02:17 | time:952s total_exs:535800 epochs:8.29\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3735 17388 558.6 5640  .3177    .3268 5.365   1  2015  9381 213.7      .1996                 4465 5751 26769 4.655\n",
            "\n",
            "03:02:28 | time:962s total_exs:541440 epochs:8.38\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3659 17005 557.7 5640  .3294    .3507 5.354   1  1985  9227 211.5      .2010                 4512 5644 26232 4.648\n",
            "\n",
            "03:02:38 | time:972s total_exs:547200 epochs:8.47\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3633 17433 575.8 5760  .3394    .3247 5.345   1  1940  9307 209.7      .2023                 4560 5572 26741 4.799\n",
            "\n",
            "03:02:48 | time:982s total_exs:552480 epochs:8.55\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3707 16016 518.5 5280  .3115    .3500 5.352   1  1979  8549  211      .2022                 4604 5686 24565 4.321\n",
            "\n",
            "03:02:58 | time:992s total_exs:558480 epochs:8.64\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3629 17735 586.4 6000  .3206    .3042 5.348   1  1953  9542 210.1      .2034                 4654 5582 27277 4.887\n",
            "\n",
            "03:03:08 | time:1002s total_exs:564120 epochs:8.73\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3654 17087 561.2 5640  .3356    .3609 5.329   1  1961  9170 206.3      .2033                 4701 5614 26257 4.677\n",
            "\n",
            "03:03:18 | time:1013s total_exs:569640 epochs:8.81\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3696 16780 544.8 5520  .3384    .3927 5.316   1  1967  8932 203.5      .2047                 4747 5663 25713 4.541\n",
            "\n",
            "03:03:28 | time:1023s total_exs:575160 epochs:8.90\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3639 16686 550.2 5520  .3489    .3606 5.281   1  1935  8873 196.6      .2062                 4793 5574 25559 4.586\n",
            "\n",
            "03:03:39 | time:1033s total_exs:581040 epochs:8.99\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3663 17806 583.3 5880  .3075    .3960 5.292   1  1966  9555 198.8      .2056                 4842 5629 27361 4.861\n",
            "\n",
            "03:03:49 | time:1043s total_exs:586680 epochs:9.08\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3689 17342 564.1 5640  .3145    .3605 5.311   1  1952  9175 202.5      .2054                 4889 5641 26517 4.701\n",
            "\n",
            "03:03:59 | time:1053s total_exs:592560 epochs:9.17\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3665 17980 588.6 5880  .3374    .3753 5.276   1  1934  9485 195.7      .2064                 4938 5599 27466 4.906\n",
            "\n",
            "03:04:09 | time:1063s total_exs:598080 epochs:9.25\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3736 17199 552.4 5520  .3291    .3945 5.281   1  1962  9030 196.6      .2078                 4984 5698 26229 4.604\n",
            "\n",
            "03:04:19 | time:1073s total_exs:603840 epochs:9.34\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3761 17868 570.1 5760  .3116    .3683 5.296   1  1971  9362 199.5      .2054                 5032 5731 27230 4.752\n",
            "\n",
            "03:04:29 | time:1083s total_exs:609480 epochs:9.43\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3672 17110 559.2 5640  .3225    .4220  5.26   1  1972  9189 192.5      .2071                 5079 5643 26300 4.66\n",
            "\n",
            "03:04:39 | time:1093s total_exs:615600 epochs:9.52\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3647 18542 610.1 6120  .3175    .3043 5.272   1  1978 10058 194.9      .2079                 5130 5625 28600 5.085\n",
            "\n",
            "03:04:49 | time:1103s total_exs:620760 epochs:9.60\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3641 15629 515.1 5160  .3232    .3947 5.254   1  1957  8403 191.3      .2084                 5173 5598 24033 4.294\n",
            "\n",
            "03:04:59 | time:1113s total_exs:626760 epochs:9.70\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3636 17990 593.7 6000  .3400    .2999 5.243   1  1945  9621 189.2      .2107                 5223 5581 27612 4.948\n",
            "\n",
            "03:05:09 | time:1124s total_exs:632760 epochs:9.79\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3661 18189 596.2 6000  .3317    .3018 5.244   1  1941  9644 189.5      .2094                 5273 5602 27832 4.969\n",
            "\n",
            "03:05:19 | time:1134s total_exs:638400 epochs:9.88\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3643 16847 554.9 5640  .3319    .4232 5.239   1  1993  9217 188.5      .2072                 5320 5636 26064 4.625\n",
            "\n",
            "03:05:30 | time:1144s total_exs:644640 epochs:9.97\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3695 18935 614.9 6240  .3077    .3019 5.209   1  1948  9983 182.9      .2102                 5372 5643 28919 5.125\n",
            "\n",
            "03:05:40 | time:1154s total_exs:650640 epochs:10.07\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3609 17954   597 6000  .3100    .3264 5.198   1  1930  9604 180.9      .2147                 5422 5539 27559 4.976\n",
            "\n",
            "03:05:50 | time:1164s total_exs:656400 epochs:10.16\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3642 17087   563 5760  .3257    .3946 5.193   1  1958  9185  180      .2134                 5470 5600 26272 4.692\n",
            "\n",
            "03:06:00 | time:1174s total_exs:662160 epochs:10.24\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3733 17695 568.8 5760  .3274    .3601 5.205   1  1961  9297 182.3      .2113                 5518 5695 26992 4.74\n",
            "\n",
            "03:06:10 | time:1184s total_exs:668040 epochs:10.34\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3680 17957 585.5 5880  .3291    .3385 5.192   1  1965  9587 179.8      .2114                 5567 5645 27544 4.88\n",
            "\n",
            "03:06:20 | time:1195s total_exs:673920 epochs:10.43\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3646 17753 584.2 5880  .3229    .2790 5.175   1  1926  9377 176.8      .2158                 5616 5573 27130 4.869\n",
            "\n",
            "03:06:31 | time:1205s total_exs:679560 epochs:10.51\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3712 17026 550.4 5640  .3197    .3928 5.178   1  1947  8932 177.3      .2140                 5663 5659 25958 4.587\n",
            "\n",
            "03:06:41 | time:1215s total_exs:685080 epochs:10.60\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3727 16875 543.4 5520  .3122    .3951 5.172   1  1973  8932 176.3      .2137                 5709 5699 25807 4.528\n",
            "\n",
            "03:06:51 | time:1225s total_exs:690480 epochs:10.68\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3722 16685 537.9 5400  .3244    .3753 5.178   1  1968  8823 177.3      .2146                 5754 5690 25507 4.483\n",
            "\n",
            "03:07:01 | time:1235s total_exs:696120 epochs:10.77\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3685 16959 552.3 5640  .3322    .3243 5.158   1  1946  8957 173.8      .2165                 5801 5631 25916 4.603\n",
            "\n",
            "03:07:11 | time:1246s total_exs:702000 epochs:10.86\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3668 17704 579.2 5880  .3315    .3601 5.136   1  1944  9381  170      .2165                 5850 5611 27085 4.827\n",
            "\n",
            "03:07:21 | time:1256s total_exs:708600 epochs:10.96\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3646 20064 660.3 6600  .3291    .2625 5.141   1  1914 10530 170.9      .2174                 5905 5560 30595 5.503\n",
            "\n",
            "03:07:31 | time:1266s total_exs:714480 epochs:11.05\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3630 17687 584.6 5880  .3192    .3244 5.143   1  1932  9414 171.3      .2161                 5954 5563 27102 4.872\n",
            "\n",
            "03:07:42 | time:1276s total_exs:720720 epochs:11.15\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3664 18910 619.3 6240  .3367    .2930 5.144   1  1931  9964 171.3      .2170                 6006 5595 28874 5.161\n",
            "\n",
            "03:07:52 | time:1286s total_exs:726480 epochs:11.24\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3666 17313 566.7 5760  .3271    .3928 5.144   1  1959  9254 171.3      .2148                 6054 5625 26567 4.723\n",
            "\n",
            "03:08:02 | time:1296s total_exs:731640 epochs:11.32\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3738 15954 512.1 5160  .3266    .3387  5.14   1  1985  8471 170.7      .2160                 6097 5723 24425 4.268\n",
            "\n",
            "03:08:12 | time:1306s total_exs:737280 epochs:11.41\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3637 16968 559.8 5640  .3260    .3682 5.105   1  1963  9160 164.8      .2197                 6144 5601 26128 4.665\n",
            "\n",
            "03:08:22 | time:1316s total_exs:743280 epochs:11.50\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3686 18387 598.6 6000  .3133    .2930 5.116   1  1944  9699 166.7      .2213                 6194 5630 28087 4.989\n",
            "\n",
            "03:08:32 | time:1327s total_exs:749400 epochs:11.59\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3654 18170 596.7 6120  .3270    .3051 5.088   1  1931  9600 162.1      .2193                 6245 5584 27770 4.973\n",
            "\n",
            "03:08:42 | time:1337s total_exs:755520 epochs:11.69\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3628 18242 603.3 6120  .3208    .3023 5.132   1  1972  9914 169.4      .2163                 6296 5601 28156 5.028\n",
            "\n",
            "03:08:53 | time:1347s total_exs:761520 epochs:11.78\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3653 18210 598.1 6000  .3145    .2821 5.097   1  1952  9732 163.5      .2211                 6346 5605 27942 4.985\n",
            "\n",
            "03:09:03 | time:1357s total_exs:767400 epochs:11.87\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3700 17797 577.1 5880  .3179    .2820  5.09   1  1959  9419 162.3      .2215                 6395 5659 27217 4.81\n",
            "\n",
            "03:09:13 | time:1367s total_exs:772680 epochs:11.95\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3742 16379 525.2 5280  .3308    .3248 5.104   1  1990  8710 164.8      .2177                 6439 5733 25089 4.377\n",
            "\n",
            "03:09:23 | time:1377s total_exs:778440 epochs:12.04\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3699 17500 567.6 5760  .3230    .3683 5.091   1  1981  9369 162.6      .2172                 6487 5680 26869 4.731\n",
            "\n",
            "03:09:33 | time:1387s total_exs:784320 epochs:12.13\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps  ups\n",
            "       1  3739 18281 586.7 5880  .3291    .3053 5.049   1  1964  9601 155.8      .2233                 6536 5702 27882 4.89\n",
            "\n",
            "03:09:43 | time:1397s total_exs:790080 epochs:12.22\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3706 17705 573.2 5760  .3335    .3250  5.06   1  1934  9237 157.6      .2225                 6584 5640 26943 4.777\n",
            "\n",
            "03:09:53 | time:1408s total_exs:795480 epochs:12.31\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3736 16600 533.1 5400  .3277    .3500 5.081   1  1975  8775 160.9      .2174                 6629 5711 25376 4.443\n",
            "\n",
            "03:10:03 | time:1418s total_exs:801480 epochs:12.40\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3679 18356 598.6 6000  .3390    .3608 5.052   1  1956  9758 156.3      .2227                 6679 5635 28114 4.989\n",
            "\n",
            "03:10:13 | time:1428s total_exs:807240 epochs:12.49\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3696 17673 573.7 5760  .3205    .2900 5.076   1  1963  9384 160.1      .2217                 6727 5659 27057 4.782\n",
            "\n",
            "03:10:23 | time:1438s total_exs:812760 epochs:12.57\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3708 17070 552.4 5520  .3273    .4235 5.075   1  1950  8978  160      .2190                 6773 5659 26049 4.604\n",
            "\n",
            "03:10:33 | time:1448s total_exs:818520 epochs:12.66\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3675 17633 575.7 5760  .3263    .2932 5.041   1  1958  9393 154.6      .2246                 6821 5633 27026 4.798\n",
            "\n",
            "03:10:44 | time:1458s total_exs:824160 epochs:12.75\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3644 17031 560.7 5640  .3267    .3046 5.048   1  1965  9180 155.7      .2236                 6868 5609 26211 4.673\n",
            "\n",
            "03:10:54 | time:1468s total_exs:829800 epochs:12.84\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3691 17019 553.2 5640  .3150    .3746 5.052   1  1989  9169 156.4      .2228                 6915 5680 26188 4.611\n",
            "\n",
            "03:11:04 | time:1478s total_exs:835200 epochs:12.92\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3652 16234 533.3 5400  .3256    .3244 5.029   1  2000  8890 152.7      .2245                 6960 5653 25124 4.445\n",
            "\n",
            "03:11:14 | time:1488s total_exs:840840 epochs:13.01\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3719 17131 552.7 5640  .3516    .2818 5.036   1  1973  9087 153.9      .2232                 7007 5692 26218 4.607\n",
            "\n",
            "03:11:24 | time:1499s total_exs:846720 epochs:13.10\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1  3679 17912 584.3 5880  .3207    .3683 5.034   1  1976  9622 153.5      .2254                 7056 5655 27535 4.869\n",
            "\n",
            "03:11:26 | max_train_time elapsed:1500.1524274349213s\n",
            "03:11:26 | Using CUDA\n",
            "03:11:26 | loading dictionary from from_scratch_model/model.dict\n",
            "03:11:26 | num words = 22419\n",
            "03:12:55 | Initialized embeddings for 16270 tokens (72.6%) from fasttext.\n",
            "03:12:55 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "03:12:55 | Loading existing model params from from_scratch_model/model\n",
            "03:12:55 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "03:12:56 | running eval: valid\n",
            "03:17:40 | eval completed in 284.21s\n",
            "03:17:40 | \u001b[1mvalid:\n",
            "    accuracy   bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb  \\\n",
            "           0 .0005791  4535 797.9 20.19 5738 .1221   .01201 4.799   1  1796 316.1 121.4      .2389                 7063 6331   \n",
            "    tps  \n",
            "   1114\n",
            "\u001b[0m\n",
            "03:17:40 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "03:17:42 | running eval: test\n",
            "03:22:29 | eval completed in 286.60s\n",
            "03:22:29 | \u001b[1mtest:\n",
            "    accuracy  bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb  \\\n",
            "           0 .001131  4883 783.8 18.35 5259 .1193   .01202  4.83   1  1812 290.9 125.2      .2353                 7063 6695   \n",
            "    tps  \n",
            "   1075\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(0.0005791),\n",
              "  'ctpb': GlobalAverageMetric(4535),\n",
              "  'ctps': GlobalTimerMetric(797.9),\n",
              "  'exps': GlobalTimerMetric(20.19),\n",
              "  'exs': SumMetric(5738),\n",
              "  'f1': F1Metric(0.1221),\n",
              "  'gpu_mem': GlobalAverageMetric(0.01201),\n",
              "  'loss': AverageMetric(4.799),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(1796),\n",
              "  'ltps': GlobalTimerMetric(316.1),\n",
              "  'ppl': PPLMetric(121.4),\n",
              "  'token_acc': AverageMetric(0.2389),\n",
              "  'total_train_updates': GlobalFixedMetric(7063),\n",
              "  'tpb': GlobalAverageMetric(6331),\n",
              "  'tps': GlobalTimerMetric(1114)},\n",
              " {'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(0.001131),\n",
              "  'ctpb': GlobalAverageMetric(4883),\n",
              "  'ctps': GlobalTimerMetric(783.8),\n",
              "  'exps': GlobalTimerMetric(18.35),\n",
              "  'exs': SumMetric(5259),\n",
              "  'f1': F1Metric(0.1193),\n",
              "  'gpu_mem': GlobalAverageMetric(0.01202),\n",
              "  'loss': AverageMetric(4.83),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(1812),\n",
              "  'ltps': GlobalTimerMetric(290.9),\n",
              "  'ppl': PPLMetric(125.2),\n",
              "  'token_acc': AverageMetric(0.2353),\n",
              "  'total_train_updates': GlobalFixedMetric(7063),\n",
              "  'tpb': GlobalAverageMetric(6695),\n",
              "  'tps': GlobalTimerMetric(1075)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF9gBLNLZx1q"
      },
      "source": [
        "# Model improve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXqzVtp0Z2NS",
        "outputId": "fdb504a7-846d-4695-8455-b004ba5ed7c6"
      },
      "source": [
        "%%time\n",
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    n_heads=16, \n",
        "    n_layers=8, \n",
        "    n_positions=512, \n",
        "    text_truncate=512,\n",
        "    label_truncate=128, \n",
        "    ffn_size=2048, \n",
        "    embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    validation_metric='ppl',\n",
        "    max_train_time=15 * 60, validation_every_n_epochs=0.25,\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    skip_generation=True,\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03:32:08 | building dictionary first...\n",
            "03:32:08 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "03:32:08 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,eval_dynamic_batching: None,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages\u001b[0m\n",
            "03:32:08 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "03:32:08 | Using CUDA\n",
            "03:32:08 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "03:32:08 | num words = 54944\n",
            "03:32:10 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "03:32:10 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "03:32:44 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "03:32:44 | Opt:\n",
            "03:32:44 |     activation: gelu\n",
            "03:32:44 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "03:32:44 |     adam_eps: 1e-08\n",
            "03:32:44 |     add_p1_after_newln: False\n",
            "03:32:44 |     aggregate_micro: False\n",
            "03:32:44 |     allow_missing_init_opts: False\n",
            "03:32:44 |     attention_dropout: 0.0\n",
            "03:32:44 |     batchsize: 12\n",
            "03:32:44 |     beam_block_full_context: True\n",
            "03:32:44 |     beam_block_list_filename: None\n",
            "03:32:44 |     beam_block_ngram: -1\n",
            "03:32:44 |     beam_context_block_ngram: -1\n",
            "03:32:44 |     beam_delay: 30\n",
            "03:32:44 |     beam_length_penalty: 0.65\n",
            "03:32:44 |     beam_min_length: 1\n",
            "03:32:44 |     beam_size: 1\n",
            "03:32:44 |     betas: '(0.9, 0.999)'\n",
            "03:32:44 |     bpe_add_prefix_space: None\n",
            "03:32:44 |     bpe_debug: False\n",
            "03:32:44 |     bpe_dropout: None\n",
            "03:32:44 |     bpe_merge: None\n",
            "03:32:44 |     bpe_vocab: None\n",
            "03:32:44 |     compute_tokenized_bleu: False\n",
            "03:32:44 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "03:32:44 |     datatype: train\n",
            "03:32:44 |     delimiter: '\\n'\n",
            "03:32:44 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "03:32:44 |     dict_endtoken: __end__\n",
            "03:32:44 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "03:32:44 |     dict_include_test: False\n",
            "03:32:44 |     dict_include_valid: False\n",
            "03:32:44 |     dict_initpath: None\n",
            "03:32:44 |     dict_language: english\n",
            "03:32:44 |     dict_loaded: True\n",
            "03:32:44 |     dict_lower: True\n",
            "03:32:44 |     dict_max_ngram_size: -1\n",
            "03:32:44 |     dict_maxexs: -1\n",
            "03:32:44 |     dict_maxtokens: -1\n",
            "03:32:44 |     dict_minfreq: 0\n",
            "03:32:44 |     dict_nulltoken: __null__\n",
            "03:32:44 |     dict_starttoken: __start__\n",
            "03:32:44 |     dict_textfields: text,labels\n",
            "03:32:44 |     dict_tokenizer: bpe\n",
            "03:32:44 |     dict_unktoken: __unk__\n",
            "03:32:44 |     display_examples: False\n",
            "03:32:44 |     download_path: None\n",
            "03:32:44 |     dropout: 0.0\n",
            "03:32:44 |     dynamic_batching: full\n",
            "03:32:44 |     embedding_projection: random\n",
            "03:32:44 |     embedding_size: 512\n",
            "03:32:44 |     embedding_type: random\n",
            "03:32:44 |     embeddings_scale: True\n",
            "03:32:44 |     eval_batchsize: None\n",
            "03:32:44 |     eval_dynamic_batching: None\n",
            "03:32:44 |     evaltask: None\n",
            "03:32:44 |     ffn_size: 2048\n",
            "03:32:44 |     force_fp16_tokens: False\n",
            "03:32:44 |     fp16: True\n",
            "03:32:44 |     fp16_impl: mem_efficient\n",
            "03:32:44 |     gpu: -1\n",
            "03:32:44 |     gradient_clip: 0.1\n",
            "03:32:44 |     hide_labels: False\n",
            "03:32:44 |     history_add_global_end_token: None\n",
            "03:32:44 |     history_reversed: False\n",
            "03:32:44 |     history_size: -1\n",
            "03:32:44 |     image_cropsize: 224\n",
            "03:32:44 |     image_mode: raw\n",
            "03:32:44 |     image_size: 256\n",
            "03:32:44 |     inference: greedy\n",
            "03:32:44 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "03:32:44 |     init_opt: None\n",
            "03:32:44 |     interactive_mode: False\n",
            "03:32:44 |     invsqrt_lr_decay_gamma: -1\n",
            "03:32:44 |     label_truncate: 128\n",
            "03:32:44 |     learn_positional_embeddings: True\n",
            "03:32:44 |     learningrate: 1e-05\n",
            "03:32:44 |     load_from_checkpoint: True\n",
            "03:32:44 |     log_every_n_secs: 10\n",
            "03:32:44 |     loglevel: info\n",
            "03:32:44 |     lr_scheduler: reduceonplateau\n",
            "03:32:44 |     lr_scheduler_decay: 0.5\n",
            "03:32:44 |     lr_scheduler_patience: 3\n",
            "03:32:44 |     max_lr_steps: -1\n",
            "03:32:44 |     max_train_time: 900.0\n",
            "03:32:44 |     metrics: default\n",
            "03:32:44 |     model: transformer/generator\n",
            "03:32:44 |     model_file: from_pretrained/model\n",
            "03:32:44 |     model_parallel: False\n",
            "03:32:44 |     momentum: 0\n",
            "03:32:44 |     multitask_weights: [1]\n",
            "03:32:44 |     mutators: None\n",
            "03:32:44 |     n_decoder_layers: -1\n",
            "03:32:44 |     n_encoder_layers: -1\n",
            "03:32:44 |     n_heads: 16\n",
            "03:32:44 |     n_layers: 8\n",
            "03:32:44 |     n_positions: 512\n",
            "03:32:44 |     n_segments: 0\n",
            "03:32:44 |     nesterov: True\n",
            "03:32:44 |     no_cuda: False\n",
            "03:32:44 |     num_epochs: -1\n",
            "03:32:44 |     nus: (0.7,)\n",
            "03:32:44 |     optimizer: mem_eff_adam\n",
            "03:32:44 |     output_scaling: 1.0\n",
            "03:32:44 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 900.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "03:32:44 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "03:32:44 |     person_tokens: False\n",
            "03:32:44 |     rank_candidates: False\n",
            "03:32:44 |     relu_dropout: 0.0\n",
            "03:32:44 |     remove_political_convos: False\n",
            "03:32:44 |     save_after_valid: False\n",
            "03:32:44 |     save_every_n_secs: -1\n",
            "03:32:44 |     share_word_embeddings: True\n",
            "03:32:44 |     short_final_eval: False\n",
            "03:32:44 |     skip_generation: True\n",
            "03:32:44 |     special_tok_lst: None\n",
            "03:32:44 |     split_lines: False\n",
            "03:32:44 |     starttime: Mar19_03-32\n",
            "03:32:44 |     task: empathetic_dialogues\n",
            "03:32:44 |     temperature: 1.0\n",
            "03:32:44 |     tensorboard_log: False\n",
            "03:32:44 |     tensorboard_logdir: None\n",
            "03:32:44 |     text_truncate: 512\n",
            "03:32:44 |     topk: 10\n",
            "03:32:44 |     topp: 0.9\n",
            "03:32:44 |     train_experiencer_only: False\n",
            "03:32:44 |     truncate: -1\n",
            "03:32:44 |     update_freq: 1\n",
            "03:32:44 |     use_reply: label\n",
            "03:32:44 |     validation_cutoff: 1.0\n",
            "03:32:44 |     validation_every_n_epochs: 0.25\n",
            "03:32:44 |     validation_every_n_secs: -1\n",
            "03:32:44 |     validation_max_exs: -1\n",
            "03:32:44 |     validation_metric: ppl\n",
            "03:32:44 |     validation_metric_mode: None\n",
            "03:32:44 |     validation_patience: 10\n",
            "03:32:44 |     validation_share_agent: False\n",
            "03:32:44 |     variant: xlm\n",
            "03:32:44 |     verbose: False\n",
            "03:32:44 |     wandb_log: False\n",
            "03:32:44 |     wandb_name: None\n",
            "03:32:44 |     wandb_project: None\n",
            "03:32:44 |     warmup_rate: 0.0001\n",
            "03:32:44 |     warmup_updates: 100\n",
            "03:32:44 |     weight_decay: None\n",
            "03:32:44 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "03:32:49 | training...\n",
            "03:32:50 | Overflow: setting loss scale to 65536.0\n",
            "03:32:59 | time:10s total_exs:3908 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9714  2635  9347   396 3908             65536  4.357    .5411 2.883 3.501e-06  1771  6283 17.86      .3910   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     35 4407 15630 3.547\n",
            "\n",
            "03:33:10 | time:20s total_exs:7064 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3021 10347 308.8 3156             65536   4.07    .5141 2.812 7e-06  1591  5449 16.65      .3988   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     70 4613 15796 3.425\n",
            "\n",
            "03:33:15 | Overflow: setting loss scale to 32768.0\n",
            "03:33:20 | time:31s total_exs:10296 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9706  2736  9256 321.6 3232             50116  4.246    .5411 2.788 1e-05  1589  5377 16.25      .3984   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    104 4325 14632 3.383\n",
            "\n",
            "03:33:30 | time:41s total_exs:13448 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2843  9450 308.1 3152             32768  4.331    .5200 2.747 1e-05  1533  5096 15.6      .4052   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    138 4377 14547 3.324\n",
            "\n",
            "03:33:39 | time:49s total_exs:16224 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2997 10144   324 2776             32768  4.095    .5200 2.731 1e-05  1658  5612 15.35      .4079   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    167 4655 15757 3.385\n",
            "\n",
            "03:33:39 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "03:33:41 | running eval: valid\n",
            "03:33:41 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "03:33:47 | eval completed in 6.43s\n",
            "03:33:47 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37608 929.7 5738    .1000 2.536 1e-05  1392 14885 12.63      .4307                  167 4909 52493\n",
            "\u001b[0m\n",
            "03:33:47 | \u001b[1;32mnew best ppl: 12.63\u001b[0m\n",
            "03:33:47 | saving best valid model: from_pretrained/model\n",
            "03:33:47 | Saving dictionary to from_pretrained/model.dict\n",
            "03:34:01 | time:72s total_exs:19484 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2966  9708 323.3 3260             32768  3.896    .5200 2.709 1e-05  1652  5405 15.01      .4123   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    200 4618 15113 3.273\n",
            "\n",
            "03:34:12 | time:82s total_exs:22412 epochs:0.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3110 10367   287 2928             32768  4.155    .5249 2.679 1e-05  1498  4993 14.56      .4126   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    234 4608 15360 3.334\n",
            "\n",
            "03:34:22 | time:93s total_exs:25776 epochs:0.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2881  9917 330.9 3364             32768  4.173    .5412 2.663 1e-05  1553  5348 14.34      .4185   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    269 4434 15265 3.443\n",
            "\n",
            "03:34:32 | time:103s total_exs:29048 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2998 10410 324.6 3272             32768  4.547    .5200 2.666 1e-05  1541  5350 14.39      .4186   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    304 4538 15760 3.473\n",
            "\n",
            "03:34:42 | time:113s total_exs:32196 epochs:0.50\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2900  9552 314.2 3148             32768  4.328    .5249 2.696 1e-05  1596  5256 14.82      .4136   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    337 4496 14808 3.294\n",
            "\n",
            "03:34:43 | time:113s total_exs:32404 epochs:0.50\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3104 10304 345.1  208             32768  3.563    .4929  2.67 1e-05  1784  5924 14.44      .4194   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    339 4888 16229 3.325\n",
            "\n",
            "03:34:43 | running eval: valid\n",
            "03:34:49 | eval completed in 6.34s\n",
            "03:34:49 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37731 932.7 5738    .1000 2.504 1e-05  1392 14934 12.23      .4361                  339 4909 52665\n",
            "\u001b[0m\n",
            "03:34:49 | \u001b[1;32mnew best ppl: 12.23 (previous best was 12.63)\u001b[0m\n",
            "03:34:49 | saving best valid model: from_pretrained/model\n",
            "03:35:03 | time:134s total_exs:35852 epochs:0.55\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3063 10973 343.1 3448             32768  4.226    .5333 2.646 1e-05  1557  5577 14.1      .4211   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    375 4619 16550 3.583\n",
            "\n",
            "03:35:14 | time:144s total_exs:38996 epochs:0.60\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2676  8995 310.8 3144             32768  4.183    .5280 2.688 1e-05  1554  5224 14.7      .4135   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    409 4230 14219 3.361\n",
            "\n",
            "03:35:24 | time:154s total_exs:42360 epochs:0.66\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3085 10310 330.7 3364             32768  4.155    .5053 2.656 1e-05  1632  5455 14.24      .4178   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    443 4717 15766 3.342\n",
            "\n",
            "03:35:34 | time:165s total_exs:45372 epochs:0.70\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3027 10611 301.6 3012             32768   4.25    .5332  2.68 1e-05  1483  5199 14.58      .4160   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    478 4511 15810 3.505\n",
            "\n",
            "03:35:44 | time:175s total_exs:48588 epochs:0.75\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2715  9367   317 3216             32768  4.145    .5076 2.665 1e-05  1558  5374 14.37      .4177   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    513 4272 14741 3.45\n",
            "\n",
            "03:35:44 | time:175s total_exs:48588 epochs:0.75\n",
            "    gpu_mem    lr  total_train_updates\n",
            "      .1000 1e-05                  513\n",
            "\n",
            "03:35:44 | running eval: valid\n",
            "03:35:50 | eval completed in 6.39s\n",
            "03:35:50 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 39315 971.9 5738    .1000 2.488 1e-05  1392 15561 12.04      .4391                  513 4909 54877\n",
            "\u001b[0m\n",
            "03:35:50 | \u001b[1;32mnew best ppl: 12.04 (previous best was 12.23)\u001b[0m\n",
            "03:35:50 | saving best valid model: from_pretrained/model\n",
            "03:36:05 | time:196s total_exs:52048 epochs:0.81\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3108 10670 339.4 3460             32768  4.073    .4929 2.671 1e-05  1673  5743 14.46      .4162   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    548 4780 16413 3.434\n",
            "\n",
            "03:36:07 | Overflow: setting loss scale to 16384.0\n",
            "03:36:15 | time:206s total_exs:55284 epochs:0.86\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9722  2833  9970 316.3 3236             20025  4.124    .5141 2.657 1e-05  1533  5395 14.26      .4165   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    584 4367 15365 3.519\n",
            "\n",
            "03:36:25 | time:216s total_exs:58508 epochs:0.91\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2953 10340 322.5 3224             16384   4.85    .5053 2.622 1e-05  1523  5331 13.76      .4239   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    619 4476 15671 3.501\n",
            "\n",
            "03:36:35 | time:226s total_exs:61852 epochs:0.96\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3024 10560 333.6 3344             16384  4.024    .5108 2.609 1e-05  1556  5431 13.59      .4217   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    654 4580 15991 3.492\n",
            "\n",
            "03:36:45 | time:236s total_exs:64780 epochs:1.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2976 10148 302.5 2928             16384   4.28    .5076 2.637 1e-05  1546  5272 13.97      .4193   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    687 4522 15419 3.41\n",
            "\n",
            "03:36:45 | running eval: valid\n",
            "03:36:51 | eval completed in 6.55s\n",
            "03:36:51 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 36941 913.2 5738    .1001 2.477 1e-05  1371 14621 11.91      .4402                  687 4835 51562\n",
            "\u001b[0m\n",
            "03:36:51 | \u001b[1;32mnew best ppl: 11.91 (previous best was 12.04)\u001b[0m\n",
            "03:36:51 | saving best valid model: from_pretrained/model\n",
            "03:37:06 | time:257s total_exs:68256 epochs:1.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2832  9988 340.6 3476             16384  4.076    .5023 2.625 1e-05  1569  5534 13.81      .4217   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    723 4401 15522 3.528\n",
            "\n",
            "03:37:16 | time:267s total_exs:71616 epochs:1.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3003 10697 332.4 3360             16384   4.18    .5412 2.627 1e-05  1533  5459 13.83      .4189   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    759 4536 16157 3.562\n",
            "\n",
            "03:37:26 | time:277s total_exs:74748 epochs:1.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2628  9063 308.6 3132             16384   4.55    .5200 2.647 1e-05  1448  4993 14.11      .4198   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    794 4075 14056 3.449\n",
            "\n",
            "03:37:36 | time:287s total_exs:77908 epochs:1.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3099 10726 312.4 3160             16384  4.209    .5280 2.608 1e-05  1497  5180 13.57      .4213   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    829 4596 15906 3.461\n",
            "\n",
            "03:37:45 | time:296s total_exs:81076 epochs:1.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3169 11250 362.8 3168             16384  4.086    .5200 2.616 1e-05  1689  5995 13.68      .4245   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    860 4857 17245 3.551\n",
            "\n",
            "03:37:45 | running eval: valid\n",
            "03:37:52 | eval completed in 6.45s\n",
            "03:37:52 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37433 925.4 5738    .1000  2.47 1e-05  1392 14816 11.82      .4416                  860 4909 52249\n",
            "\u001b[0m\n",
            "03:37:52 | \u001b[1;32mnew best ppl: 11.82 (previous best was 11.91)\u001b[0m\n",
            "03:37:52 | saving best valid model: from_pretrained/model\n",
            "03:38:06 | time:317s total_exs:84400 epochs:1.31\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2932 10277 332.9 3324             16384  4.047    .5141 2.611 1e-05  1587  5562 13.61      .4209   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    895 4519 15838 3.506\n",
            "\n",
            "03:38:16 | time:327s total_exs:87496 epochs:1.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2880  9706 306.8 3096             16384  4.248    .5412  2.62 1e-05  1510  5088 13.73      .4201   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    929 4391 14794 3.37\n",
            "\n",
            "03:38:26 | time:337s total_exs:90836 epochs:1.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3037 10246 331.4 3340             16384  4.064    .5108 2.623 1e-05  1611  5434 13.77      .4235   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    963 4647 15680 3.374\n",
            "\n",
            "03:38:36 | time:347s total_exs:93920 epochs:1.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3115 10532 306.7 3084             16384  4.298    .5200 2.635 1e-05  1528  5165 13.94      .4221   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    997 4642 15698 3.382\n",
            "\n",
            "03:38:46 | time:357s total_exs:97280 epochs:1.51\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2917 10393 332.5 3360             16384  4.226    .5332 2.584 1e-05  1566  5579 13.25      .4254   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1033 4483 15972 3.563\n",
            "\n",
            "03:38:46 | time:357s total_exs:97280 epochs:1.51\n",
            "    gpu_mem    lr  total_train_updates\n",
            "      .1000 1e-05                 1033\n",
            "\n",
            "03:38:46 | running eval: valid\n",
            "03:38:52 | eval completed in 6.18s\n",
            "03:38:52 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38896 961.5 5738    .1001 2.463 1e-05  1392 15395 11.74      .4424                 1033 4909 54292\n",
            "\u001b[0m\n",
            "03:38:52 | \u001b[1;32mnew best ppl: 11.74 (previous best was 11.82)\u001b[0m\n",
            "03:38:52 | saving best valid model: from_pretrained/model\n",
            "03:39:07 | time:378s total_exs:100408 epochs:1.55\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3140 10394 304.6 3128             16384   4.59    .5023 2.636 1e-05  1622  5372 13.95      .4211   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1067 4762 15766 3.311\n",
            "\n",
            "03:39:17 | time:388s total_exs:103680 epochs:1.60\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2866  9652 324.1 3272             16384  4.192    .5200 2.596 1e-05  1549  5218 13.42      .4251   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1101 4415 14870 3.368\n",
            "\n",
            "03:39:27 | time:398s total_exs:107000 epochs:1.66\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2896 10056 329.4 3320             16384  4.255    .5200 2.629 1e-05  1586  5508 13.86      .4210   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1136 4482 15563 3.473\n",
            "\n",
            "03:39:37 | time:408s total_exs:110444 epochs:1.71\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2886  9637 338.3 3444             16384  4.073    .5249 2.598 1e-05  1643  5487 13.44      .4240   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1170 4529 15125 3.34\n",
            "\n",
            "03:39:47 | time:417s total_exs:113496 epochs:1.76\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2849 10075 327.1 3052             16384  3.995    .5200 2.593 1e-05  1599  5656 13.36      .4238   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1203 4448 15731 3.537\n",
            "\n",
            "03:39:47 | running eval: valid\n",
            "03:39:53 | eval completed in 6.30s\n",
            "03:39:53 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 38050 940.6 5738    .1000 2.458 1e-05  1371 15060 11.68      .4432                 1203 4835 53111\n",
            "\u001b[0m\n",
            "03:39:53 | \u001b[1;32mnew best ppl: 11.68 (previous best was 11.74)\u001b[0m\n",
            "03:39:53 | saving best valid model: from_pretrained/model\n",
            "03:40:08 | time:438s total_exs:116528 epochs:1.80\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3022 10308 295.4 3032             16384  4.554    .5333 2.606 1e-05  1375  4691 13.55      .4252   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1238 4398 14999 3.411\n",
            "\n",
            "03:40:18 | time:448s total_exs:119504 epochs:1.85\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2987 10158 297.7 2976             16384  4.281    .5412 2.602 1e-05  1550  5272 13.5      .4225   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1272 4537 15430 3.401\n",
            "\n",
            "03:40:28 | time:458s total_exs:122952 epochs:1.90\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2899  9782 342.2 3448             16384  4.102    .5200 2.592 1e-05  1651  5573 13.36      .4263   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1306 4550 15355 3.375\n",
            "\n",
            "03:40:38 | time:469s total_exs:126036 epochs:1.95\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2995 10287 302.6 3084             16384  4.181    .5412 2.592 1e-05  1505  5168 13.35      .4250   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1341 4500 15456 3.435\n",
            "\n",
            "03:40:48 | time:479s total_exs:129204 epochs:2.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2996 10078 313.4 3168             16384  4.308    .5200 2.589 1e-05  1591  5351 13.32      .4254   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1375 4587 15429 3.364\n",
            "\n",
            "03:40:50 | time:480s total_exs:129688 epochs:2.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2907  9916 330.1  484             16384  3.754    .4929 2.497 1e-05  1684  5744 12.15      .4425   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1380 4591 15661 3.413\n",
            "\n",
            "03:40:50 | running eval: valid\n",
            "03:40:56 | eval completed in 6.16s\n",
            "03:40:56 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3571 38785 958.8 5738    .1000 2.453 1e-05  1413 15351 11.63      .4440                 1380 4984 54137\n",
            "\u001b[0m\n",
            "03:40:56 | \u001b[1;32mnew best ppl: 11.63 (previous best was 11.68)\u001b[0m\n",
            "03:40:56 | saving best valid model: from_pretrained/model\n",
            "03:41:10 | time:501s total_exs:133012 epochs:2.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2874  9763 332.1 3324             16384  4.165    .5053 2.554 1e-05  1568  5327 12.86      .4289   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1414 4442 15090 3.397\n",
            "\n",
            "03:41:20 | time:511s total_exs:135796 epochs:2.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2970  9998 275.6 2784             16384   4.92    .5317 2.577 1e-05  1424  4791 13.15      .4264   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1448 4394 14789 3.366\n",
            "\n",
            "03:41:30 | time:521s total_exs:139488 epochs:2.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2928 10561 369.8 3692             16384  4.298    .5024 2.581 1e-05  1639  5911 13.22      .4266   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1484 4568 16472 3.607\n",
            "\n",
            "03:41:40 | time:531s total_exs:142700 epochs:2.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2831 10130 319.3 3212             16384  4.378    .5053 2.573 1e-05  1523  5452 13.1      .4254   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1520 4354 15582 3.579\n",
            "\n",
            "03:41:50 | time:541s total_exs:145852 epochs:2.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3021 10318 307.6 3152             16384  4.248    .5280 2.592 1e-05  1532  5232 13.36      .4267   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1555 4553 15550 3.416\n",
            "\n",
            "03:41:50 | time:541s total_exs:145852 epochs:2.26\n",
            "    gpu_mem    lr  total_train_updates\n",
            "      .1000 1e-05                 1555\n",
            "\n",
            "03:41:50 | running eval: valid\n",
            "03:41:57 | eval completed in 6.56s\n",
            "03:41:57 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38024   940 5738    .1000 2.449 1e-05  1392 15050 11.58      .4442                 1555 4909 53074\n",
            "\u001b[0m\n",
            "03:41:57 | \u001b[1;32mnew best ppl: 11.58 (previous best was 11.63)\u001b[0m\n",
            "03:41:57 | saving best valid model: from_pretrained/model\n",
            "03:42:11 | time:562s total_exs:149276 epochs:2.31\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2771  9975 342.4 3424             16384  4.238    .5249 2.565 1e-05  1542  5553 12.99      .4283   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1591 4313 15529 3.601\n",
            "\n",
            "03:42:21 | time:572s total_exs:152704 epochs:2.36\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2901 10023 338.4 3428             16384  3.959    .5158 2.558 1e-05  1619  5593 12.91      .4307   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1626 4520 15616 3.455\n",
            "\n",
            "03:42:32 | time:582s total_exs:155732 epochs:2.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3039  9967 300.9 3028             16384   4.56    .5200 2.547 1e-05  1502  4926 12.77      .4349   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1659 4542 14893 3.28\n",
            "\n",
            "03:42:42 | time:593s total_exs:158892 epochs:2.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2761  9905 306.4 3160             16384   4.27    .5332 2.592 1e-05  1521  5456 13.36      .4250   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1696 4281 15361 3.588\n",
            "\n",
            "03:42:52 | time:602s total_exs:162032 epochs:2.51\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3068 10686 321.7 3140             16384  4.132    .5141 2.564 1e-05  1546  5383 12.99      .4282   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1730 4614 16070 3.483\n",
            "\n",
            "03:42:52 | running eval: valid\n",
            "03:42:58 | eval completed in 6.42s\n",
            "03:42:58 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 37182 919.2 5738    .1000 2.445 1e-05  1351 14717 11.54      .4449                 1730 4764 51899\n",
            "\u001b[0m\n",
            "03:42:58 | \u001b[1;32mnew best ppl: 11.54 (previous best was 11.58)\u001b[0m\n",
            "03:42:58 | saving best valid model: from_pretrained/model\n",
            "03:43:12 | time:623s total_exs:165460 epochs:2.56\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3011 10745 339.8 3428             16384  4.266    .5141  2.55 1e-05  1517  5413 12.81      .4322   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1766 4527 16158 3.569\n",
            "\n",
            "03:43:23 | time:633s total_exs:168620 epochs:2.61\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2974  9900 309.4 3160             16384  4.748    .5412 2.561 1e-05  1644  5472 12.95      .4318   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1800 4618 15372 3.329\n",
            "\n",
            "03:43:33 | time:643s total_exs:172196 epochs:2.66\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2942 10305 357.9 3576             16384  4.398    .5200 2.553 1e-05  1665  5833 12.85      .4282   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1835 4607 16138 3.503\n",
            "\n",
            "03:43:43 | time:654s total_exs:175116 epochs:2.71\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2934  9723 284.6 2920             16384  4.347    .5317 2.564 1e-05  1424  4718 12.98      .4309   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1869 4358 14441 3.314\n",
            "\n",
            "03:43:52 | time:663s total_exs:178200 epochs:2.76\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2862  9751 328.3 3084             16384    4.2    .5317 2.578 1e-05  1606  5471 13.17      .4282   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1901 4468 15223 3.407\n",
            "\n",
            "03:43:52 | running eval: valid\n",
            "03:43:59 | eval completed in 6.26s\n",
            "03:43:59 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38301 946.8 5738    .1001 2.443 1e-05  1392 15159 11.5      .4445                 1901 4909 53460\n",
            "\u001b[0m\n",
            "03:43:59 | \u001b[1;32mnew best ppl: 11.5 (previous best was 11.54)\u001b[0m\n",
            "03:43:59 | saving best valid model: from_pretrained/model\n",
            "03:44:13 | time:684s total_exs:181432 epochs:2.81\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3065 10393 322.3 3232             16384  4.205    .5249 2.565 1e-05  1613  5468 12.99      .4257   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1935 4678 15861 3.391\n",
            "\n",
            "03:44:23 | time:694s total_exs:184996 epochs:2.86\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3082 10799 346.9 3564             16384  4.337    .5200  2.53 1e-05  1619  5672 12.55      .4320   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1971 4701 16471 3.504\n",
            "\n",
            "03:44:34 | time:704s total_exs:188148 epochs:2.91\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2704  9163 314.1 3152             16384  4.209    .5200 2.547 1e-05  1546  5239 12.77      .4302   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2005 4251 14402 3.388\n",
            "\n",
            "03:44:44 | time:715s total_exs:191284 epochs:2.96\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2785  9275 307.1 3136             16384  4.108    .5200 2.543 1e-05  1551  5163 12.72      .4313   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2039 4336 14439 3.33\n",
            "\n",
            "03:44:54 | time:724s total_exs:194388 epochs:3.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2944  9642 317.7 3104             16384  4.021    .5053 2.545 1e-05  1563  5120 12.75      .4319   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2071 4507 14762 3.276\n",
            "\n",
            "03:44:54 | running eval: valid\n",
            "03:45:00 | eval completed in 6.30s\n",
            "03:45:00 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 37917 937.3 5738    .1000 2.442 1e-05  1351 15008 11.49      .4446                 2071 4764 52925\n",
            "\u001b[0m\n",
            "03:45:00 | \u001b[1;32mnew best ppl: 11.49 (previous best was 11.5)\u001b[0m\n",
            "03:45:00 | saving best valid model: from_pretrained/model\n",
            "03:45:14 | time:745s total_exs:197300 epochs:3.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3101 10705 287.2 2912             16384  4.528    .4929 2.565 1e-05  1457  5031   13      .4277   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2106 4559 15736 3.452\n",
            "\n",
            "03:45:24 | time:755s total_exs:200788 epochs:3.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2904 10250   342 3488             16384  4.286    .5200 2.527 1e-05  1588  5606 12.51      .4353   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2142 4492 15856 3.53\n",
            "\n",
            "03:45:35 | time:765s total_exs:204036 epochs:3.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2869 10089 317.3 3248             16384   4.22    .5332 2.537 1e-05  1556  5471 12.64      .4321   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2178 4424 15560 3.517\n",
            "\n",
            "03:45:45 | time:776s total_exs:207412 epochs:3.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3003 10502 337.3 3376             16384  4.127    .5332 2.524 1e-05  1623  5676 12.47      .4326   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2213 4626 16178 3.498\n",
            "\n",
            "03:45:54 | time:785s total_exs:210572 epochs:3.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3050 10539 330.9 3160             16384  4.549    .5200  2.52 1e-05  1536  5309 12.42      .4336   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2246 4586 15849 3.456\n",
            "\n",
            "03:45:54 | running eval: valid\n",
            "03:46:01 | eval completed in 6.27s\n",
            "03:46:01 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38605 954.3 5738    .1000 2.439 1e-05  1392 15280 11.46      .4447                 2246 4909 53885\n",
            "\u001b[0m\n",
            "03:46:01 | \u001b[1;32mnew best ppl: 11.46 (previous best was 11.49)\u001b[0m\n",
            "03:46:01 | saving best valid model: from_pretrained/model\n",
            "03:46:15 | time:806s total_exs:213768 epochs:3.31\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2989 10071 316.7 3196             16384   4.25    .5249 2.555 1e-05  1625  5476 12.87      .4318   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2280 4615 15546 3.369\n",
            "\n",
            "03:46:25 | time:816s total_exs:217104 epochs:3.36\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3140 10718 325.3 3336             16384  4.218    .5200  2.51 1e-05  1565  5343 12.31      .4361   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2315 4705 16062 3.414\n",
            "\n",
            "03:46:35 | time:826s total_exs:220700 epochs:3.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2710  9532 351.3 3596             16384  4.249    .5333  2.55 1e-05  1652  5811 12.81      .4274   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2351 4362 15343 3.517\n",
            "\n",
            "03:46:46 | time:837s total_exs:223736 epochs:3.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2999 10207 295.2 3036             16384  4.632    .5075  2.54 1e-05  1536  5229 12.68      .4307   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2386 4535 15436 3.404\n",
            "\n",
            "03:46:56 | time:846s total_exs:226780 epochs:3.51\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2930  9951 313.3 3044             16384  4.233    .5053 2.532 1e-05  1512  5136 12.58      .4294   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2419 4442 15087 3.397\n",
            "\n",
            "03:46:56 | running eval: valid\n",
            "03:47:02 | eval completed in 6.31s\n",
            "03:47:02 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 39285 971.2 5738    .1000 2.436 1e-05  1371 15549 11.43      .4451                 2419 4835 54834\n",
            "\u001b[0m\n",
            "03:47:02 | \u001b[1;32mnew best ppl: 11.43 (previous best was 11.46)\u001b[0m\n",
            "03:47:02 | saving best valid model: from_pretrained/model\n",
            "03:47:16 | time:867s total_exs:230176 epochs:3.56\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2930 10476 337.3 3396             16384    4.2    .5200 2.517 1e-05  1576  5633 12.4      .4349   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2455 4506 16110 3.576\n",
            "\n",
            "03:47:26 | time:877s total_exs:233520 epochs:3.61\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3059 10923 331.7 3344             16384  4.262    .5108 2.519 1e-05  1589  5674 12.41      .4345   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2491 4648 16597 3.571\n",
            "\n",
            "03:47:36 | time:887s total_exs:237072 epochs:3.67\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3021 10748 351.1 3552             16384  4.436    .4733 2.519 1e-05  1631  5804 12.41      .4315   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2527 4652 16552 3.559\n",
            "\n",
            "03:47:46 | time:897s total_exs:240360 epochs:3.72\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2941  9969 327.8 3288             18793  4.198    .5317 2.527 1e-05  1627  5516 12.52      .4264   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2561 4568 15485 3.39\n",
            "\n",
            "03:47:49 | max_train_time elapsed:900.2007784843445s\n",
            "03:47:50 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "03:47:50 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "03:47:50 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,eval_dynamic_batching: None,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "03:47:50 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "03:47:50 | Using CUDA\n",
            "03:47:50 | loading dictionary from from_pretrained/model.dict\n",
            "03:47:50 | num words = 54944\n",
            "03:47:51 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "03:47:51 | Loading existing model params from from_pretrained/model\n",
            "03:47:54 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "03:47:56 | running eval: valid\n",
            "03:48:03 | eval completed in 6.60s\n",
            "03:48:03 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37046 915.8 5738   .08855 2.436 1e-05  1392 14663 11.43      .4451                 2419 4909 51709\n",
            "\u001b[0m\n",
            "03:48:03 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "03:48:06 | running eval: test\n",
            "03:48:12 | eval completed in 6.10s\n",
            "03:48:12 | \u001b[1mtest:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3706 39473 903.5 5259   .08857 2.459 1e-05  1377 14664 11.69      .4433                 2419 5082 54137\n",
            "\u001b[0m\n",
            "CPU times: user 10min 53s, sys: 4min 10s, total: 15min 4s\n",
            "Wall time: 16min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUlAL8xtabxH"
      },
      "source": [
        "# Check prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1miNK6RLaebV",
        "outputId": "58c6ff1a-2308-4962-b5d9-f6275fa68a4c"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03:57:53 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "03:57:53 | Using CUDA\n",
            "03:57:53 | loading dictionary from from_pretrained/model.dict\n",
            "03:57:53 | num words = 54944\n",
            "03:57:55 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "03:57:55 | Loading existing model params from from_pretrained/model\n",
            "03:58:06 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "03:58:07 | Opt:\n",
            "03:58:07 |     activation: gelu\n",
            "03:58:07 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "03:58:07 |     adam_eps: 1e-08\n",
            "03:58:07 |     add_p1_after_newln: False\n",
            "03:58:07 |     aggregate_micro: False\n",
            "03:58:07 |     allow_missing_init_opts: False\n",
            "03:58:07 |     attention_dropout: 0.0\n",
            "03:58:07 |     batchsize: 12\n",
            "03:58:07 |     beam_block_full_context: True\n",
            "03:58:07 |     beam_block_list_filename: None\n",
            "03:58:07 |     beam_block_ngram: -1\n",
            "03:58:07 |     beam_context_block_ngram: -1\n",
            "03:58:07 |     beam_delay: 30\n",
            "03:58:07 |     beam_length_penalty: 0.65\n",
            "03:58:07 |     beam_min_length: 1\n",
            "03:58:07 |     beam_size: 1\n",
            "03:58:07 |     betas: '[0.9, 0.999]'\n",
            "03:58:07 |     bpe_add_prefix_space: None\n",
            "03:58:07 |     bpe_debug: False\n",
            "03:58:07 |     bpe_dropout: None\n",
            "03:58:07 |     bpe_merge: None\n",
            "03:58:07 |     bpe_vocab: None\n",
            "03:58:07 |     compute_tokenized_bleu: False\n",
            "03:58:07 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "03:58:07 |     datatype: train\n",
            "03:58:07 |     delimiter: '\\n'\n",
            "03:58:07 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "03:58:07 |     dict_endtoken: __end__\n",
            "03:58:07 |     dict_file: from_pretrained/model.dict\n",
            "03:58:07 |     dict_include_test: False\n",
            "03:58:07 |     dict_include_valid: False\n",
            "03:58:07 |     dict_initpath: None\n",
            "03:58:07 |     dict_language: english\n",
            "03:58:07 |     dict_loaded: True\n",
            "03:58:07 |     dict_lower: True\n",
            "03:58:07 |     dict_max_ngram_size: -1\n",
            "03:58:07 |     dict_maxexs: -1\n",
            "03:58:07 |     dict_maxtokens: -1\n",
            "03:58:07 |     dict_minfreq: 0\n",
            "03:58:07 |     dict_nulltoken: __null__\n",
            "03:58:07 |     dict_starttoken: __start__\n",
            "03:58:07 |     dict_textfields: text,labels\n",
            "03:58:07 |     dict_tokenizer: bpe\n",
            "03:58:07 |     dict_unktoken: __unk__\n",
            "03:58:07 |     display_add_fields: \n",
            "03:58:07 |     display_examples: False\n",
            "03:58:07 |     download_path: None\n",
            "03:58:07 |     dropout: 0.0\n",
            "03:58:07 |     dynamic_batching: full\n",
            "03:58:07 |     embedding_projection: random\n",
            "03:58:07 |     embedding_size: 512\n",
            "03:58:07 |     embedding_type: random\n",
            "03:58:07 |     embeddings_scale: True\n",
            "03:58:07 |     eval_batchsize: None\n",
            "03:58:07 |     eval_dynamic_batching: None\n",
            "03:58:07 |     evaltask: None\n",
            "03:58:07 |     ffn_size: 2048\n",
            "03:58:07 |     force_fp16_tokens: True\n",
            "03:58:07 |     fp16: True\n",
            "03:58:07 |     fp16_impl: mem_efficient\n",
            "03:58:07 |     gpu: -1\n",
            "03:58:07 |     gradient_clip: 0.1\n",
            "03:58:07 |     hide_labels: False\n",
            "03:58:07 |     history_add_global_end_token: None\n",
            "03:58:07 |     history_reversed: False\n",
            "03:58:07 |     history_size: -1\n",
            "03:58:07 |     image_cropsize: 224\n",
            "03:58:07 |     image_mode: raw\n",
            "03:58:07 |     image_size: 256\n",
            "03:58:07 |     inference: greedy\n",
            "03:58:07 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "03:58:07 |     init_opt: None\n",
            "03:58:07 |     interactive_mode: False\n",
            "03:58:07 |     invsqrt_lr_decay_gamma: -1\n",
            "03:58:07 |     label_truncate: 128\n",
            "03:58:07 |     learn_positional_embeddings: True\n",
            "03:58:07 |     learningrate: 1e-05\n",
            "03:58:07 |     log_every_n_secs: 10\n",
            "03:58:07 |     loglevel: info\n",
            "03:58:07 |     lr_scheduler: reduceonplateau\n",
            "03:58:07 |     lr_scheduler_decay: 0.5\n",
            "03:58:07 |     lr_scheduler_patience: 3\n",
            "03:58:07 |     max_lr_steps: -1\n",
            "03:58:07 |     max_train_time: 900.0\n",
            "03:58:07 |     metrics: default\n",
            "03:58:07 |     model: transformer/generator\n",
            "03:58:07 |     model_file: from_pretrained/model\n",
            "03:58:07 |     model_parallel: False\n",
            "03:58:07 |     momentum: 0\n",
            "03:58:07 |     multitask_weights: [1]\n",
            "03:58:07 |     mutators: None\n",
            "03:58:07 |     n_decoder_layers: -1\n",
            "03:58:07 |     n_encoder_layers: -1\n",
            "03:58:07 |     n_heads: 16\n",
            "03:58:07 |     n_layers: 8\n",
            "03:58:07 |     n_positions: 512\n",
            "03:58:07 |     n_segments: 0\n",
            "03:58:07 |     nesterov: True\n",
            "03:58:07 |     no_cuda: False\n",
            "03:58:07 |     num_epochs: -1\n",
            "03:58:07 |     num_examples: 2\n",
            "03:58:07 |     nus: [0.7]\n",
            "03:58:07 |     optimizer: mem_eff_adam\n",
            "03:58:07 |     output_scaling: 1.0\n",
            "03:58:07 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model', 'num_examples': '2', 'skip_generation': False}\"\n",
            "03:58:07 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "03:58:07 |     person_tokens: False\n",
            "03:58:07 |     rank_candidates: False\n",
            "03:58:07 |     relu_dropout: 0.0\n",
            "03:58:07 |     remove_political_convos: False\n",
            "03:58:07 |     save_after_valid: False\n",
            "03:58:07 |     save_every_n_secs: -1\n",
            "03:58:07 |     share_word_embeddings: True\n",
            "03:58:07 |     short_final_eval: False\n",
            "03:58:07 |     skip_generation: False\n",
            "03:58:07 |     special_tok_lst: None\n",
            "03:58:07 |     split_lines: False\n",
            "03:58:07 |     starttime: Mar19_03-32\n",
            "03:58:07 |     task: empathetic_dialogues\n",
            "03:58:07 |     temperature: 1.0\n",
            "03:58:07 |     tensorboard_log: False\n",
            "03:58:07 |     tensorboard_logdir: None\n",
            "03:58:07 |     text_truncate: 512\n",
            "03:58:07 |     topk: 10\n",
            "03:58:07 |     topp: 0.9\n",
            "03:58:07 |     train_experiencer_only: False\n",
            "03:58:07 |     truncate: -1\n",
            "03:58:07 |     update_freq: 1\n",
            "03:58:07 |     use_reply: label\n",
            "03:58:07 |     validation_cutoff: 1.0\n",
            "03:58:07 |     validation_every_n_epochs: 0.25\n",
            "03:58:07 |     validation_every_n_secs: -1\n",
            "03:58:07 |     validation_max_exs: -1\n",
            "03:58:07 |     validation_metric: ppl\n",
            "03:58:07 |     validation_metric_mode: None\n",
            "03:58:07 |     validation_patience: 10\n",
            "03:58:07 |     validation_share_agent: False\n",
            "03:58:07 |     variant: xlm\n",
            "03:58:07 |     verbose: False\n",
            "03:58:07 |     wandb_log: False\n",
            "03:58:07 |     wandb_name: None\n",
            "03:58:07 |     wandb_project: None\n",
            "03:58:07 |     warmup_rate: 0.0001\n",
            "03:58:07 |     warmup_updates: 100\n",
            "03:58:07 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that sounds scary ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good . i hope you ' re okay .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFzgrtfEootc",
        "outputId": "0acab8cd-b765-44f2-ae96-0b19b9ae3749"
      },
      "source": [
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "Interactive.main(\n",
        "\n",
        "    model_file='/content/from_pretrained/model'\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03:59:32 | \u001b[33mOverriding opt[\"model_file\"] to /content/from_pretrained/model (previously: from_pretrained/model)\u001b[0m\n",
            "03:59:32 | Using CUDA\n",
            "03:59:32 | loading dictionary from /content/from_pretrained/model.dict\n",
            "03:59:32 | num words = 54944\n",
            "03:59:32 | TransformerGenerator: full interactive mode on.\n",
            "03:59:34 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "03:59:34 | Loading existing model params from /content/from_pretrained/model\n",
            "03:59:35 | Opt:\n",
            "03:59:35 |     activation: gelu\n",
            "03:59:35 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "03:59:35 |     adam_eps: 1e-08\n",
            "03:59:35 |     add_p1_after_newln: False\n",
            "03:59:35 |     aggregate_micro: False\n",
            "03:59:35 |     allow_missing_init_opts: False\n",
            "03:59:35 |     attention_dropout: 0.0\n",
            "03:59:35 |     batchsize: 12\n",
            "03:59:35 |     beam_block_full_context: True\n",
            "03:59:35 |     beam_block_list_filename: None\n",
            "03:59:35 |     beam_block_ngram: -1\n",
            "03:59:35 |     beam_context_block_ngram: -1\n",
            "03:59:35 |     beam_delay: 30\n",
            "03:59:35 |     beam_length_penalty: 0.65\n",
            "03:59:35 |     beam_min_length: 1\n",
            "03:59:35 |     beam_size: 1\n",
            "03:59:35 |     betas: '[0.9, 0.999]'\n",
            "03:59:35 |     bpe_add_prefix_space: None\n",
            "03:59:35 |     bpe_debug: False\n",
            "03:59:35 |     bpe_dropout: None\n",
            "03:59:35 |     bpe_merge: None\n",
            "03:59:35 |     bpe_vocab: None\n",
            "03:59:35 |     compute_tokenized_bleu: False\n",
            "03:59:35 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "03:59:35 |     datatype: train\n",
            "03:59:35 |     delimiter: '\\n'\n",
            "03:59:35 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "03:59:35 |     dict_endtoken: __end__\n",
            "03:59:35 |     dict_file: /content/from_pretrained/model.dict\n",
            "03:59:35 |     dict_include_test: False\n",
            "03:59:35 |     dict_include_valid: False\n",
            "03:59:35 |     dict_initpath: None\n",
            "03:59:35 |     dict_language: english\n",
            "03:59:35 |     dict_loaded: True\n",
            "03:59:35 |     dict_lower: True\n",
            "03:59:35 |     dict_max_ngram_size: -1\n",
            "03:59:35 |     dict_maxexs: -1\n",
            "03:59:35 |     dict_maxtokens: -1\n",
            "03:59:35 |     dict_minfreq: 0\n",
            "03:59:35 |     dict_nulltoken: __null__\n",
            "03:59:35 |     dict_starttoken: __start__\n",
            "03:59:35 |     dict_textfields: text,labels\n",
            "03:59:35 |     dict_tokenizer: bpe\n",
            "03:59:35 |     dict_unktoken: __unk__\n",
            "03:59:35 |     display_add_fields: \n",
            "03:59:35 |     display_examples: False\n",
            "03:59:35 |     display_prettify: False\n",
            "03:59:35 |     download_path: None\n",
            "03:59:35 |     dropout: 0.0\n",
            "03:59:35 |     dynamic_batching: full\n",
            "03:59:35 |     embedding_projection: random\n",
            "03:59:35 |     embedding_size: 512\n",
            "03:59:35 |     embedding_type: random\n",
            "03:59:35 |     embeddings_scale: True\n",
            "03:59:35 |     eval_batchsize: None\n",
            "03:59:35 |     eval_dynamic_batching: None\n",
            "03:59:35 |     evaltask: None\n",
            "03:59:35 |     ffn_size: 2048\n",
            "03:59:35 |     force_fp16_tokens: True\n",
            "03:59:35 |     fp16: True\n",
            "03:59:35 |     fp16_impl: mem_efficient\n",
            "03:59:35 |     gpu: -1\n",
            "03:59:35 |     gradient_clip: 0.1\n",
            "03:59:35 |     hide_labels: False\n",
            "03:59:35 |     history_add_global_end_token: None\n",
            "03:59:35 |     history_reversed: False\n",
            "03:59:35 |     history_size: -1\n",
            "03:59:35 |     image_cropsize: 224\n",
            "03:59:35 |     image_mode: raw\n",
            "03:59:35 |     image_size: 256\n",
            "03:59:35 |     inference: greedy\n",
            "03:59:35 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "03:59:35 |     init_opt: None\n",
            "03:59:35 |     interactive_mode: True\n",
            "03:59:35 |     interactive_task: True\n",
            "03:59:35 |     invsqrt_lr_decay_gamma: -1\n",
            "03:59:35 |     label_truncate: 128\n",
            "03:59:35 |     learn_positional_embeddings: True\n",
            "03:59:35 |     learningrate: 1e-05\n",
            "03:59:35 |     local_human_candidates_file: None\n",
            "03:59:35 |     log_every_n_secs: 10\n",
            "03:59:35 |     log_keep_fields: all\n",
            "03:59:35 |     loglevel: info\n",
            "03:59:35 |     lr_scheduler: reduceonplateau\n",
            "03:59:35 |     lr_scheduler_decay: 0.5\n",
            "03:59:35 |     lr_scheduler_patience: 3\n",
            "03:59:35 |     max_lr_steps: -1\n",
            "03:59:35 |     max_train_time: 900.0\n",
            "03:59:35 |     metrics: default\n",
            "03:59:35 |     model: transformer/generator\n",
            "03:59:35 |     model_file: /content/from_pretrained/model\n",
            "03:59:35 |     model_parallel: False\n",
            "03:59:35 |     momentum: 0\n",
            "03:59:35 |     multitask_weights: [1]\n",
            "03:59:35 |     mutators: None\n",
            "03:59:35 |     n_decoder_layers: -1\n",
            "03:59:35 |     n_encoder_layers: -1\n",
            "03:59:35 |     n_heads: 16\n",
            "03:59:35 |     n_layers: 8\n",
            "03:59:35 |     n_positions: 512\n",
            "03:59:35 |     n_segments: 0\n",
            "03:59:35 |     nesterov: True\n",
            "03:59:35 |     no_cuda: False\n",
            "03:59:35 |     num_epochs: -1\n",
            "03:59:35 |     nus: [0.7]\n",
            "03:59:35 |     optimizer: mem_eff_adam\n",
            "03:59:35 |     outfile: \n",
            "03:59:35 |     output_scaling: 1.0\n",
            "03:59:35 |     override: \"{'model_file': '/content/from_pretrained/model'}\"\n",
            "03:59:35 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "03:59:35 |     person_tokens: False\n",
            "03:59:35 |     rank_candidates: False\n",
            "03:59:35 |     relu_dropout: 0.0\n",
            "03:59:35 |     remove_political_convos: False\n",
            "03:59:35 |     save_after_valid: False\n",
            "03:59:35 |     save_every_n_secs: -1\n",
            "03:59:35 |     save_format: conversations\n",
            "03:59:35 |     share_word_embeddings: True\n",
            "03:59:35 |     short_final_eval: False\n",
            "03:59:35 |     single_turn: False\n",
            "03:59:35 |     skip_generation: True\n",
            "03:59:35 |     special_tok_lst: None\n",
            "03:59:35 |     split_lines: False\n",
            "03:59:35 |     starttime: Mar19_03-32\n",
            "03:59:35 |     task: empathetic_dialogues\n",
            "03:59:35 |     temperature: 1.0\n",
            "03:59:35 |     tensorboard_log: False\n",
            "03:59:35 |     tensorboard_logdir: None\n",
            "03:59:35 |     text_truncate: 512\n",
            "03:59:35 |     topk: 10\n",
            "03:59:35 |     topp: 0.9\n",
            "03:59:35 |     train_experiencer_only: False\n",
            "03:59:35 |     truncate: -1\n",
            "03:59:35 |     update_freq: 1\n",
            "03:59:35 |     use_reply: label\n",
            "03:59:35 |     validation_cutoff: 1.0\n",
            "03:59:35 |     validation_every_n_epochs: 0.25\n",
            "03:59:35 |     validation_every_n_secs: -1\n",
            "03:59:35 |     validation_max_exs: -1\n",
            "03:59:35 |     validation_metric: ppl\n",
            "03:59:35 |     validation_metric_mode: None\n",
            "03:59:35 |     validation_patience: 10\n",
            "03:59:35 |     validation_share_agent: False\n",
            "03:59:35 |     variant: xlm\n",
            "03:59:35 |     verbose: False\n",
            "03:59:35 |     wandb_log: False\n",
            "03:59:35 |     wandb_name: None\n",
            "03:59:35 |     wandb_project: None\n",
            "03:59:35 |     warmup_rate: 0.0001\n",
            "03:59:35 |     warmup_updates: 100\n",
            "03:59:35 |     weight_decay: None\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "03:59:35 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m Hello\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m doing well , how about you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m just fine the weather is cool here\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m glad to hear that .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how about you where are you \n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m please be specific\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m where excactly\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m ok, let's change the topic\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m ??\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m DONE\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m in the midwest .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [DONE]\n",
            "CHAT DONE \n",
            "\n",
            "... preparing new chat... \n",
            "\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m Hi, I am Neo\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi neo , i am neo\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m no i am neo who are you\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi neo , i am neo\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m waht\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi neo , i am neo\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi neo , i am neo\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m EXIT\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi neo , i am neo\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Gs5Mvpaofb"
      },
      "source": [
        "**for command line**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNlXJLIZarUp"
      },
      "source": [
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRpCo-0fa0YV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvjvyPI0a03t"
      },
      "source": [
        "# Self made dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVGFv9a6a7cG"
      },
      "source": [
        "%%time\n",
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc7HShm6bVhW"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZQgPo1eaA0t"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSFYU_FdaECT"
      },
      "source": [
        "website\n",
        "https://parl.ai/docs/"
      ]
    },
    {
      "source": [
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MlsbaEHyaDpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}